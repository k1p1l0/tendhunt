---
phase: 20-board-minutes-signals
plan: 03
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - apps/workers/board-minutes/src/stages/01-extract-signals.ts
  - apps/workers/board-minutes/src/stages/02-deduplicate.ts
  - apps/workers/board-minutes/src/signal-engine.ts
autonomous: true

must_haves:
  truths:
    - "Stage 1 finds Tier 0 buyers with unprocessed BoardDocuments and extracts signals via Claude Haiku"
    - "Text chunking splits documents into 4000-char chunks with 200-char overlap at paragraph/sentence boundaries"
    - "Claude Haiku returns JSON array of signals that gets validated, normalized, and upserted to signals collection"
    - "Stage 2 deduplicates signals within same buyer + same type within 30-day window"
    - "Processed BoardDocuments are marked with signalExtractionStatus: extracted"
    - "Pipeline completes a full cycle with both stages"
  artifacts:
    - path: "apps/workers/board-minutes/src/stages/01-extract-signals.ts"
      provides: "Signal extraction: buyer iteration, doc fetching, chunking, Claude Haiku call, signal upsert"
      min_lines: 150
    - path: "apps/workers/board-minutes/src/stages/02-deduplicate.ts"
      provides: "Cross-chunk signal deduplication per buyer"
      min_lines: 40
  key_links:
    - from: "apps/workers/board-minutes/src/stages/01-extract-signals.ts"
      to: "Claude Haiku API"
      via: "anthropic.messages.create with extraction prompt"
      pattern: "messages\\.create"
    - from: "apps/workers/board-minutes/src/stages/01-extract-signals.ts"
      to: "MongoDB signals collection"
      via: "bulkWrite upsert"
      pattern: "bulkWrite"
    - from: "apps/workers/board-minutes/src/signal-engine.ts"
      to: "apps/workers/board-minutes/src/stages/01-extract-signals.ts"
      via: "STAGE_FUNCTIONS registry"
      pattern: "extractSignals"
---

<objective>
Implement the two pipeline stages: signal extraction from BoardDocuments via Claude Haiku (Stage 1) and cross-chunk deduplication (Stage 2). Wire them into the pipeline engine, replacing the placeholder stages from Plan 01.

Purpose: This is the core intelligence logic -- the only truly new code in the worker. Everything else is boilerplate.
Output: Working signal extraction pipeline that processes BoardDocuments and produces Signal records.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-board-minutes-signals/20-01-SUMMARY.md
@apps/workers/board-minutes/src/types.ts
@apps/workers/board-minutes/src/signal-engine.ts
@apps/workers/board-minutes/src/db/signal-jobs.ts
@apps/workers/enrichment/src/stages/05-personnel.ts
@/Users/kirillkozak/Projects/board-minutes-intelligence/tendhunt-api/src/services/signal-extraction.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Stage 1 - Signal extraction from BoardDocuments</name>
  <files>
    apps/workers/board-minutes/src/stages/01-extract-signals.ts
  </files>
  <action>
    Create the core signal extraction stage. This is the most complex file in the worker. Follow enrichment Stage 5 (personnel) for the Claude Haiku call pattern and the reference project for chunking/parsing.

    **Function signature:** `export async function extractSignals(db: Db, env: Env, job: SignalJobDoc, maxItems: number): Promise<{ processed: number; errors: number; done: boolean }>`

    **Buyer query:**
    Find Tier 0 buyers (have `dataSourceId`) that have BoardDocuments with `extractionStatus: "extracted"` AND `signalExtractionStatus: { $ne: "extracted" }` (not yet processed for signals). Use cursor-based pagination with `job.cursor` as the `_id` lower bound, sorted by `_id`. Limit to `maxItems` buyers per run.

    Implementation steps:
    1. Query buyers with dataSourceId that have at least one qualifying BoardDocument (aggregate with $lookup or two-step: find qualifying doc buyerIds, then find buyers)
    2. For each buyer (up to maxItems), fetch their BoardDocuments where `extractionStatus: "extracted"` AND `signalExtractionStatus: { $ne: "extracted" }` AND `textContent` exists. Limit 5 most recent by meetingDate.
    3. For each document:
       a. Chunk text using `chunkText()` (4000 chars, 200 overlap, break at paragraph/sentence boundaries -- port from reference project's `chunkText` method)
       b. For each chunk, call Claude Haiku with the extraction prompt (adapted from reference project for Anthropic SDK)
       c. Parse and validate the JSON response using `parseSignals()` (handle markdown code blocks, validate required fields, normalize types)
       d. For each valid signal, build a Signal document with: `buyerId`, `boardDocumentId`, `organizationName` (buyer.name), `signalType`, `title`, `insight` (from summary), `source` (document sourceUrl), `sourceDate` (document meetingDate), `sector` (buyer.sector or buyer.orgType), `confidence`, `quote` (truncated to 200 chars), `entities`
       e. Upsert signals via `bulkWrite` with compound filter: `{ buyerId, boardDocumentId, signalType, title }` for dedup
       f. Mark the BoardDocument as processed: `signalExtractionStatus: "extracted"`
    4. On error per document, set `signalExtractionStatus: "failed"` and log but continue
    5. Update job cursor after each buyer batch
    6. Use `p-limit(2)` for Claude API concurrency (same as enrichment personnel stage)

    **Claude Haiku call pattern (from enrichment Stage 5):**
    ```typescript
    import Anthropic from "@anthropic-ai/sdk";
    import pLimit from "p-limit";

    const anthropic = new Anthropic({ apiKey: env.ANTHROPIC_API_KEY });
    const limit = pLimit(2);

    const response = await anthropic.messages.create({
      model: "claude-haiku-4-5-20250401",
      max_tokens: 2000,
      system: SYSTEM_PROMPT,
      messages: [{ role: "user", content: userPrompt }],
    });
    ```

    **System prompt:** Use the adapted prompt from the research document (SYSTEM_PROMPT and EXTRACTION_PROMPT constants). The extraction prompt template has placeholders: `{org_name}`, `{sector}`, `{meeting_date}`, `{chunk_text}`.

    **Add a `title` field requirement to the extraction prompt.** The reference project only extracts `summary` but our Signal model needs `title` (5-10 words) AND `insight` (maps to summary, 1-2 sentences). Update the prompt to ask for both.

    **chunkText() function:** Port from reference project's `chunkText` method:
    - If text <= 4000 chars, return [text]
    - Split at paragraph breaks (\n\n) first, then sentence ends (. ? !)
    - 200-char overlap between chunks

    **parseSignals() function:** Port from reference project's `parseSignals` method:
    - Handle markdown code blocks (```json ... ```)
    - JSON.parse the trimmed result
    - Validate each signal: must have signal_type (string), summary (string, > 10 chars)
    - Normalize: clamp confidence 0-1, default to 0.5 if missing, validate signalType against the 6 enum values (default to "PROJECTS"), truncate quote to 200 chars, ensure entities arrays exist

    **500ms sleep between chunks** to avoid API rate issues (same as reference project).
  </action>
  <verify>
    Run `cd apps/workers/board-minutes && npx tsc --noEmit` to verify TypeScript compiles.
    Check the file contains: chunkText function, parseSignals function, Claude Haiku call, bulkWrite upsert, signalExtractionStatus update.
  </verify>
  <done>
    Stage 1 queries Tier 0 buyers with unprocessed BoardDocuments, chunks text at 4000 chars with 200 overlap, calls Claude Haiku for signal extraction, validates/normalizes JSON, upserts Signal records with buyerId + boardDocumentId link, and marks documents as processed. Uses p-limit(2) concurrency and cursor-based resume.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Stage 2 - Deduplication and wire stages into engine</name>
  <files>
    apps/workers/board-minutes/src/stages/02-deduplicate.ts
    apps/workers/board-minutes/src/signal-engine.ts
  </files>
  <action>
    **Stage 2: Deduplication (`02-deduplicate.ts`):**

    Function signature: `export async function deduplicateSignals(db: Db, env: Env, job: SignalJobDoc, maxItems: number): Promise<{ processed: number; errors: number; done: boolean }>`

    For each buyer (cursor-paginated, up to maxItems):
    1. Fetch all signals for this buyer, sorted by sourceDate descending
    2. Group by signalType
    3. Within each type group, compare signals within a 30-day window:
       - Extract keywords from title+insight (lowercase, strip punctuation, words > 3 chars, sorted, take top 5)
       - If two signals share the same keyword key AND are within 30 days, keep the one with higher confidence, delete the other
    4. Delete duplicates via `bulkWrite` with `deleteOne` operations
    5. Update job cursor after each buyer

    The keyword extraction and dedup logic is ported from the reference project's `deduplicateSignals()` and `extractKeywords()` methods.

    This stage is relatively lightweight compared to Stage 1 (no API calls, just MongoDB queries and in-memory comparison).

    **Wire stages into signal-engine.ts:**

    Replace the placeholder stage functions in signal-engine.ts:
    - Import `extractSignals` from `./stages/01-extract-signals`
    - Import `deduplicateSignals` from `./stages/02-deduplicate`
    - Update `STAGE_FUNCTIONS` record to use the real implementations:
    ```typescript
    const STAGE_FUNCTIONS: Record<SignalIngestStage, StageFn> = {
      extract_signals: extractSignals,
      deduplicate: deduplicateSignals,
    };
    ```
    - Remove the inline placeholder functions
  </action>
  <verify>
    Run `cd apps/workers/board-minutes && npx tsc --noEmit` to verify TypeScript compiles.
    Verify signal-engine.ts imports both stage functions and maps them in STAGE_FUNCTIONS.
  </verify>
  <done>
    Stage 2 deduplicates signals per buyer within 30-day windows by signalType + keyword similarity. Both stages are wired into the pipeline engine. Worker can now run a full pipeline cycle: extract signals from BoardDocuments, then deduplicate.
  </done>
</task>

</tasks>

<verification>
1. `cd apps/workers/board-minutes && npx tsc --noEmit` -- no errors
2. Stage 1 file contains chunkText, parseSignals, Claude Haiku call, bulkWrite, signalExtractionStatus update
3. Stage 2 file contains keyword extraction and dedup logic
4. signal-engine.ts imports and registers both real stages (no placeholders)
5. Full pipeline cycle would: find buyers -> fetch docs -> chunk -> extract via Claude -> upsert signals -> mark docs -> deduplicate
</verification>

<success_criteria>
Both stages are implemented with all the signal extraction logic ported from the reference project (adapted for Anthropic SDK). The pipeline engine runs both stages in sequence. Type-checks pass.
</success_criteria>

<output>
After completion, create `.planning/phases/20-board-minutes-signals/20-03-SUMMARY.md`
</output>
