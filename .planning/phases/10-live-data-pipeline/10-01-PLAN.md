---
phase: 10-live-data-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - workers/data-sync/package.json
  - workers/data-sync/tsconfig.json
  - workers/data-sync/wrangler.toml
  - workers/data-sync/.gitignore
  - workers/data-sync/src/types.ts
  - workers/data-sync/src/db/client.ts
  - workers/data-sync/src/db/contracts.ts
  - workers/data-sync/src/db/buyers.ts
  - workers/data-sync/src/db/sync-jobs.ts
  - workers/data-sync/src/mappers/ocds-mapper.ts
  - workers/data-sync/src/sync-engine.ts
autonomous: true

must_haves:
  truths:
    - "Worker project compiles with wrangler and has mongodb dependency"
    - "OCDS mapper converts raw API releases to contract documents matching existing Contract schema"
    - "Sync engine detects never-synced state and enters backfill mode"
    - "Sync engine saves cursor position after each page for crash-safe resume"
    - "Sync engine transitions from backfilling to syncing when no more pages remain"
    - "Contract upsert is idempotent on {source, noticeId} compound key"
    - "Buyer auto-extraction creates new buyer records from contract data using $setOnInsert"
  artifacts:
    - path: "workers/data-sync/package.json"
      provides: "Worker project dependencies (mongodb ^6.15.0)"
      contains: "mongodb"
    - path: "workers/data-sync/wrangler.toml"
      provides: "Worker config with nodejs_compat_v2 flag and hourly cron"
      contains: "nodejs_compat_v2"
    - path: "workers/data-sync/src/types.ts"
      provides: "OcdsRelease, SyncJob, MappedContract, MappedBuyer interfaces"
      contains: "SyncJob"
    - path: "workers/data-sync/src/db/client.ts"
      provides: "Native MongoDB driver connection with maxPoolSize:1"
      contains: "MongoClient"
    - path: "workers/data-sync/src/db/contracts.ts"
      provides: "bulkWrite upsert on {source, noticeId}"
      contains: "bulkWrite"
    - path: "workers/data-sync/src/db/buyers.ts"
      provides: "Buyer auto-extraction with $setOnInsert upsert"
      contains: "setOnInsert"
    - path: "workers/data-sync/src/db/sync-jobs.ts"
      provides: "SyncJob CRUD: getOrCreate, updateProgress, markComplete"
      contains: "syncJobs"
    - path: "workers/data-sync/src/mappers/ocds-mapper.ts"
      provides: "OCDS release to contract mapper with CPV sector classification"
      contains: "CPV_SECTOR_MAP"
    - path: "workers/data-sync/src/sync-engine.ts"
      provides: "Core sync loop: backfill detection, chunked processing, cursor save"
      contains: "processSyncJob"
  key_links:
    - from: "workers/data-sync/src/sync-engine.ts"
      to: "workers/data-sync/src/db/sync-jobs.ts"
      via: "getOrCreateSyncJob + updateSyncProgress"
      pattern: "getOrCreateSyncJob|updateSyncProgress"
    - from: "workers/data-sync/src/sync-engine.ts"
      to: "workers/data-sync/src/db/contracts.ts"
      via: "upsertContracts with mapped releases"
      pattern: "upsertContracts"
    - from: "workers/data-sync/src/sync-engine.ts"
      to: "workers/data-sync/src/mappers/ocds-mapper.ts"
      via: "mapOcdsToContract for each release"
      pattern: "mapOcdsToContract"
---

<objective>
Scaffold the Cloudflare Worker project for the live data pipeline and implement all core infrastructure: TypeScript types mirroring existing Mongoose schemas, native MongoDB driver connection, OCDS-to-contract mapper (port from Phase 2), sync job persistence for resumable backfill, contract/buyer upsert operations, and the core sync engine loop.

Purpose: Establish the complete data layer and processing engine so Plan 10-02 only needs to add API clients and wire the scheduled handler entry point.
Output: A compilable Worker project under `workers/data-sync/` with all DB operations, mapper, types, and sync engine ready for integration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-live-data-pipeline/10-RESEARCH.md
@.planning/phases/02-data-pipeline/02-01-SUMMARY.md
@src/models/contract.ts
@src/models/buyer.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scaffold Worker project with wrangler config, package.json, types, and MongoDB client</name>
  <files>
    workers/data-sync/package.json
    workers/data-sync/tsconfig.json
    workers/data-sync/wrangler.toml
    workers/data-sync/.gitignore
    workers/data-sync/src/types.ts
    workers/data-sync/src/db/client.ts
    workers/data-sync/src/index.ts
  </files>
  <action>
    Create `workers/data-sync/` directory as a standalone Worker project (NOT inside the Next.js app, NOT importing from `src/`). This follows the existing pattern at `landing/workers/blog-proxy/`.

    **package.json:**
    - name: "tendhunt-data-sync"
    - private: true
    - scripts: { "dev": "wrangler dev", "deploy": "wrangler deploy" }
    - dependencies: { "mongodb": "^6.15.0" }
    - devDependencies: { "wrangler": "^4.4.0", "@cloudflare/workers-types": "^4.20250205.0", "typescript": "^5.7.0" }

    **wrangler.toml:**
    ```toml
    name = "tendhunt-data-sync"
    main = "src/index.ts"
    compatibility_date = "2025-03-20"
    compatibility_flags = ["nodejs_compat_v2"]

    [triggers]
    crons = ["0 * * * *"]

    # MONGODB_URI stored as secret: wrangler secret put MONGODB_URI
    ```
    Do NOT set `[limits]` -- Cloudflare applies max CPU time automatically for cron triggers.

    **tsconfig.json:**
    ```json
    {
      "compilerOptions": {
        "target": "ES2022",
        "module": "ES2022",
        "moduleResolution": "bundler",
        "lib": ["ES2022"],
        "types": ["@cloudflare/workers-types"],
        "strict": true,
        "noEmit": true,
        "esModuleInterop": true,
        "skipLibCheck": true,
        "outDir": "dist"
      },
      "include": ["src/**/*.ts"]
    }
    ```

    **.gitignore:**
    ```
    node_modules/
    dist/
    .wrangler/
    .dev.vars
    ```

    **src/types.ts** -- Define all shared interfaces matching existing Mongoose schemas (plain objects, NOT Mongoose types):

    ```typescript
    import type { ObjectId } from "mongodb";

    // OCDS Release type (all nested fields optional for resilience)
    export interface OcdsRelease {
      ocid?: string;
      id?: string;
      date?: string;
      tag?: string[];
      language?: string;
      initiationType?: string;
      tender?: {
        id?: string;
        title?: string;
        description?: string;
        status?: string;
        value?: { amount?: number; currency?: string };
        minValue?: { amount?: number; currency?: string };
        tenderPeriod?: { startDate?: string; endDate?: string };
        classification?: { id?: string; description?: string; scheme?: string };
        items?: Array<{
          id?: string;
          description?: string;
          classification?: { id?: string; description?: string; scheme?: string };
        }>;
      };
      parties?: Array<{
        id?: string;
        name?: string;
        roles?: string[];
        address?: {
          streetAddress?: string;
          locality?: string;
          region?: string;
          postalCode?: string;
          countryName?: string;
        };
        contactPoint?: {
          name?: string;
          email?: string;
          telephone?: string;
        };
      }>;
      buyer?: { id?: string; name?: string };
      awards?: Array<{
        id?: string;
        title?: string;
        status?: string;
        date?: string;
        value?: { amount?: number; currency?: string };
        suppliers?: Array<{ id?: string; name?: string }>;
      }>;
    }

    // Matches existing Contract Mongoose schema in src/models/contract.ts
    export interface MappedContract {
      ocid: string | null;
      noticeId: string;
      source: "FIND_A_TENDER" | "CONTRACTS_FINDER";
      sourceUrl: string | null;
      title: string;
      description: string | null;
      status: "OPEN" | "CLOSED" | "AWARDED" | "CANCELLED";
      stage: "PLANNING" | "TENDER" | "AWARD";
      buyerName: string;
      buyerOrg: string | null;
      buyerRegion: string | null;
      cpvCodes: string[];
      sector: string | undefined;
      valueMin: number | null;
      valueMax: number | null;
      currency: string;
      publishedDate: Date | null;
      deadlineDate: Date | null;
      rawData: unknown;
    }

    // Matches existing Buyer Mongoose schema in src/models/buyer.ts
    export interface MappedBuyer {
      name: string;
      orgId: string;
      sector?: string;
      region?: string;
      website?: string;
      description?: string;
      contractCount: number;
      contacts: never[];
    }

    // Sync job document for tracking backfill/sync progress
    export interface SyncJob {
      _id?: ObjectId;
      source: "FIND_A_TENDER" | "CONTRACTS_FINDER";
      status: "backfilling" | "syncing" | "error";
      cursor: string | null;
      backfillStartDate: string;
      lastSyncedDate: string | null;
      totalFetched: number;
      lastRunAt: Date;
      lastRunFetched: number;
      lastRunErrors: number;
      errorLog: string[];
      createdAt: Date;
      updatedAt: Date;
    }

    // Worker environment bindings
    export interface Env {
      MONGODB_URI: string;
      BACKFILL_START_DATE?: string;
    }
    ```

    **src/db/client.ts** -- Native MongoDB driver connection for Workers:
    ```typescript
    import { MongoClient, type Db } from "mongodb";

    let client: MongoClient | null = null;

    export async function getDb(mongoUri: string): Promise<Db> {
      if (!client) {
        client = new MongoClient(mongoUri, {
          maxPoolSize: 1,
          minPoolSize: 0,
          serverSelectionTimeoutMS: 5000,
          connectTimeoutMS: 10000,
        });
      }
      await client.connect();
      return client.db("tendhunt");
    }

    export async function closeDb(): Promise<void> {
      if (client) {
        await client.close();
        client = null;
      }
    }
    ```

    **src/index.ts** -- Minimal placeholder entry point (Plan 10-02 wires the full handler):
    ```typescript
    import type { Env } from "./types";

    export default {
      async scheduled(
        controller: ScheduledController,
        env: Env,
        ctx: ExecutionContext
      ): Promise<void> {
        console.log("Data sync triggered (not yet implemented)");
      },
    } satisfies ExportedHandler<Env>;
    ```

    After creating files, run `cd workers/data-sync && npm install` to install dependencies and generate lockfile.
  </action>
  <verify>
    1. `ls workers/data-sync/package.json workers/data-sync/wrangler.toml workers/data-sync/tsconfig.json` -- all exist
    2. `cd workers/data-sync && npx tsc --noEmit` -- TypeScript compiles without errors
    3. `grep "nodejs_compat_v2" workers/data-sync/wrangler.toml` -- compatibility flag present
    4. `grep '"mongodb"' workers/data-sync/package.json` -- mongodb dependency listed
  </verify>
  <done>
    Worker project scaffolded under `workers/data-sync/` with wrangler config (hourly cron, nodejs_compat_v2), mongodb dependency installed, TypeScript interfaces matching existing Mongoose schemas defined, and native MongoDB client with serverless-optimized pool settings.
  </done>
</task>

<task type="auto">
  <name>Task 2: Port OCDS mapper and implement DB operations (contracts, buyers, sync jobs)</name>
  <files>
    workers/data-sync/src/mappers/ocds-mapper.ts
    workers/data-sync/src/db/contracts.ts
    workers/data-sync/src/db/buyers.ts
    workers/data-sync/src/db/sync-jobs.ts
  </files>
  <action>
    **src/mappers/ocds-mapper.ts** -- Port the OCDS mapper from Phase 2 (`scripts/lib/ocds-mapper.ts` in git history) as a pure function returning plain objects instead of Mongoose types. Include:
    - Full `CPV_SECTOR_MAP` dictionary (all 35+ CPV 2-digit division codes from Phase 2)
    - `deriveSectorFromCpv(cpvCode?: string): string | undefined` helper
    - `mapStatus(tenderStatus?: string)` mapping to "OPEN" | "CLOSED" | "AWARDED" | "CANCELLED"
    - `mapStage(tags?: string[])` mapping to "PLANNING" | "TENDER" | "AWARD"
    - `mapOcdsToContract(release: OcdsRelease, source: "FIND_A_TENDER" | "CONTRACTS_FINDER"): MappedContract`
      - Extract buyer from `release.parties` (role "buyer") or fallback to `release.buyer`
      - Extract CPV codes from `release.tender.items[].classification.id` + `release.tender.classification.id`
      - Map sourceUrl: FaT = `https://www.find-tender.service.gov.uk/Notice/${release.id}`, CF = `https://www.contractsfinder.service.gov.uk/Notice/${release.id}`
      - Default buyerName to "Unknown" if not found
      - Include `buyerOrg` from buyer party `id` field (maps to existing Buyer schema field)
      - Use null-safe access (`?.`) for ALL optional OCDS fields
    - Import types from `../types` (OcdsRelease, MappedContract)

    **src/db/contracts.ts** -- Contract upsert using native MongoDB driver:
    ```typescript
    import type { Db, BulkWriteResult } from "mongodb";
    import type { MappedContract } from "../types";

    export async function upsertContracts(
      db: Db,
      contracts: MappedContract[]
    ): Promise<BulkWriteResult> {
      if (contracts.length === 0) {
        return { insertedCount: 0, matchedCount: 0, modifiedCount: 0, deletedCount: 0, upsertedCount: 0, upsertedIds: {}, ok: 1 } as unknown as BulkWriteResult;
      }
      const collection = db.collection("contracts");
      const ops = contracts.map((doc) => ({
        updateOne: {
          filter: { source: doc.source, noticeId: doc.noticeId },
          update: {
            $set: { ...doc, updatedAt: new Date() },
            $setOnInsert: { createdAt: new Date() },
          },
          upsert: true,
        },
      }));
      return collection.bulkWrite(ops, { ordered: false });
    }
    ```
    This matches the existing compound unique index `{ source: 1, noticeId: 1 }` in the Contract Mongoose model.

    **src/db/buyers.ts** -- Buyer auto-extraction from contract data:
    - `autoExtractBuyers(db: Db, contracts: MappedContract[]): Promise<number>`
    - Collect unique buyers from contracts by `buyerOrg || slugify(buyerName)` as `orgId`
    - Use `$setOnInsert` to only populate fields on first creation (don't overwrite enriched buyer data)
    - Use `$inc: { contractCount: 1 }` to increment contract count on existing buyers
    - Return upserted count
    - Helper `slugify(name: string): string` to generate stable orgId from buyer name (lowercase, replace spaces with hyphens, strip non-alphanumeric)

    **src/db/sync-jobs.ts** -- SyncJob CRUD operations:
    - `getOrCreateSyncJob(db: Db, source: SyncJob["source"], backfillStartDate: string): Promise<SyncJob>` -- findOne by source, create if not exists with status "backfilling", cursor null, totalFetched 0
    - `updateSyncProgress(db: Db, jobId: ObjectId, update: { cursor: string | null; fetched: number; errors: number; totalFetched: number }): Promise<void>` -- updateOne with $set for cursor, lastRunAt, lastRunFetched, lastRunErrors, totalFetched, updatedAt; also $push errorLog entries (keep last 100 with $slice)
    - `markSyncComplete(db: Db, jobId: ObjectId): Promise<void>` -- set status to "syncing", clear cursor, set lastSyncedDate to current ISO string
    - `markSyncError(db: Db, jobId: ObjectId, error: string): Promise<void>` -- set status to "error", push error to errorLog
    - All functions import from `../types` (SyncJob type)
  </action>
  <verify>
    1. `cd workers/data-sync && npx tsc --noEmit` -- all new files compile without errors
    2. `grep "CPV_SECTOR_MAP" workers/data-sync/src/mappers/ocds-mapper.ts` -- CPV map present
    3. `grep "bulkWrite" workers/data-sync/src/db/contracts.ts` -- upsert pattern present
    4. `grep "setOnInsert" workers/data-sync/src/db/buyers.ts` -- auto-extraction pattern present
    5. `grep "getOrCreateSyncJob" workers/data-sync/src/db/sync-jobs.ts` -- sync job CRUD present
  </verify>
  <done>
    OCDS mapper ported from Phase 2 with full CPV sector map and null-safe access. Contract upsert on {source, noticeId} compound key. Buyer auto-extraction with $setOnInsert for non-destructive upsert. SyncJob CRUD with cursor persistence, error logging (capped at 100), and status transitions.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement core sync engine with backfill detection, chunked processing, and cursor resume</name>
  <files>
    workers/data-sync/src/sync-engine.ts
  </files>
  <action>
    Create the core sync engine that orchestrates the full sync pipeline for a single data source. This is the heart of the Worker -- Plan 10-02 will call this from the scheduled handler via API client functions.

    **src/sync-engine.ts:**

    ```typescript
    import type { Db } from "mongodb";
    import type { OcdsRelease, SyncJob, MappedContract, Env } from "./types";
    import { mapOcdsToContract } from "./mappers/ocds-mapper";
    import { upsertContracts } from "./db/contracts";
    import { autoExtractBuyers } from "./db/buyers";
    import { getOrCreateSyncJob, updateSyncProgress, markSyncComplete, markSyncError } from "./db/sync-jobs";
    ```

    Define the `FetchPageResult` interface that API clients must return:
    ```typescript
    export interface FetchPageResult {
      releases: OcdsRelease[];
      nextCursor: string | null;  // null means no more pages
    }
    ```

    Define the `FetchPageFn` type that the sync engine accepts (dependency injection for API clients):
    ```typescript
    export type FetchPageFn = (params: {
      cursor: string | null;
      dateFrom?: string;
    }) => Promise<FetchPageResult>;
    ```

    Implement `processSyncJob`:
    ```typescript
    export async function processSyncJob(
      db: Db,
      source: SyncJob["source"],
      fetchPage: FetchPageFn,
      backfillStartDate: string,
      maxItemsPerRun: number
    ): Promise<{ fetched: number; errors: number; done: boolean }>
    ```

    Logic:
    1. Call `getOrCreateSyncJob(db, source, backfillStartDate)` to get or create the sync job
    2. Determine mode: if `job.status === "backfilling"` use cursor-based pagination; if `job.status === "syncing"` use `lastSyncedDate` as dateFrom filter
    3. If `job.status === "error"`, reset to previous mode (backfilling if cursor exists, syncing otherwise) to retry
    4. Loop: while `fetched < maxItemsPerRun`:
       a. Call `fetchPage({ cursor: currentCursor, dateFrom: isBackfill ? undefined : job.lastSyncedDate })`
       b. If `releases.length === 0`, mark done and break
       c. Map releases: for each release, wrap `mapOcdsToContract(release, source)` in try/catch. Failed mappings increment error count and add message to errorMessages array. Successful mappings added to batch array.
       d. If batch has contracts, call `upsertContracts(db, batch)` and `autoExtractBuyers(db, batch)`
       e. Update cursor to `nextCursor`
       f. **CRITICAL: Save progress after EVERY page** -- call `updateSyncProgress(db, job._id!, { cursor: currentCursor, fetched, errors, totalFetched: job.totalFetched + fetched })` with accumulated error messages
       g. If `nextCursor === null`, mark done and break
    5. If done AND was backfilling: call `markSyncComplete(db, job._id!)` to transition to "syncing" status
    6. If any error during the loop (not per-release, but fatal like DB connection loss): call `markSyncError(db, job._id!, error.message)` and rethrow
    7. Return `{ fetched, errors, done }`

    Key design decisions:
    - The engine is agnostic of API specifics -- it receives a `fetchPage` function (dependency injection). Plan 10-02 creates FaT and CF specific implementations.
    - Progress is saved after EVERY page (not just at the end) for crash safety. If the Worker hits CPU limit mid-batch, the next invocation picks up from the last saved cursor.
    - Per-release try/catch ensures individual bad records don't kill the batch (proven pattern from Phase 2 Plan 02-02).
    - `bulkWrite({ ordered: false })` continues on individual document errors.
    - Error log in SyncJob is capped at 100 entries via $slice to prevent unbounded growth.
  </action>
  <verify>
    1. `cd workers/data-sync && npx tsc --noEmit` -- compiles without errors
    2. `grep "processSyncJob" workers/data-sync/src/sync-engine.ts` -- main function exported
    3. `grep "FetchPageFn" workers/data-sync/src/sync-engine.ts` -- dependency injection type exported
    4. `grep "updateSyncProgress" workers/data-sync/src/sync-engine.ts` -- progress saved in loop
    5. `grep "markSyncComplete" workers/data-sync/src/sync-engine.ts` -- backfill-to-sync transition
  </verify>
  <done>
    Core sync engine implemented with: backfill detection (never-synced = backfilling, completed = syncing), chunked processing with configurable maxItemsPerRun, cursor-based resume via SyncJob persistence after every page, per-release error resilience, buyer auto-extraction from each batch, and clean status transitions (backfilling -> syncing, error recovery). Engine uses dependency injection for API clients (FetchPageFn type) so Plan 10-02 only needs to implement the fetch functions.
  </done>
</task>

</tasks>

<verification>
1. `cd workers/data-sync && npx tsc --noEmit` -- entire project compiles
2. All source files exist: `ls workers/data-sync/src/{types,sync-engine,index}.ts workers/data-sync/src/db/{client,contracts,buyers,sync-jobs}.ts workers/data-sync/src/mappers/ocds-mapper.ts`
3. `grep "nodejs_compat_v2" workers/data-sync/wrangler.toml` -- critical compatibility flag
4. `grep "mongodb" workers/data-sync/package.json` -- driver dependency
5. OCDS mapper has full CPV_SECTOR_MAP (35+ entries)
6. Sync engine exports processSyncJob and FetchPageFn for Plan 10-02 integration
</verification>

<success_criteria>
- Worker project under `workers/data-sync/` compiles with TypeScript
- All types mirror existing Mongoose schemas (Contract, Buyer, SyncJob)
- OCDS mapper ported from Phase 2 with full CPV sector classification
- MongoDB native driver client with serverless pool settings (maxPoolSize:1)
- Contract upsert on {source, noticeId} matches existing compound unique index
- Buyer auto-extraction creates first-class records with $setOnInsert
- SyncJob CRUD supports full lifecycle (create, progress update, complete, error)
- Sync engine loop processes pages with crash-safe cursor persistence
- All modules are pure/decoupled -- no API-specific code in the engine
</success_criteria>

<output>
After completion, create `.planning/phases/10-live-data-pipeline/10-01-SUMMARY.md`
</output>
