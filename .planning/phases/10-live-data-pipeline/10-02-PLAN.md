---
phase: 10-live-data-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - workers/data-sync/src/api-clients/rate-limiter.ts
  - workers/data-sync/src/api-clients/fat-client.ts
  - workers/data-sync/src/api-clients/cf-client.ts
  - workers/data-sync/src/index.ts
autonomous: true
user_setup:
  - service: mongodb-atlas
    why: "Worker needs MONGODB_URI connection string to write data"
    env_vars:
      - name: MONGODB_URI
        source: "MongoDB Atlas Dashboard -> Database -> Connect -> Drivers -> connection string (same as .env.local)"
    dashboard_config:
      - task: "Add Worker IP to Atlas Network Access (or use 0.0.0.0/0 for Cloudflare Workers)"
        location: "MongoDB Atlas -> Network Access -> Add IP Address"
  - service: cloudflare-workers
    why: "Worker must be deployed and MONGODB_URI secret set"
    env_vars:
      - name: MONGODB_URI
        source: "Run: cd workers/data-sync && wrangler secret put MONGODB_URI"

must_haves:
  truths:
    - "Find a Tender API client fetches paginated OCDS releases following links.next URLs"
    - "Contracts Finder API client fetches paginated releases using bare cursor tokens"
    - "Rate limiter enforces ~6 req/min with 10s delay between requests"
    - "Rate limiter retries on 429/403/503 with exponential backoff reading Retry-After header"
    - "FaT client fetches tender and award stages separately (API rejects comma-separated stages)"
    - "CF client fetches tender,award stages in single request (comma-separated supported)"
    - "Worker scheduled handler processes FaT (5400 items) then CF (3600 items) sequentially"
    - "Worker connects to MongoDB, runs both sync jobs, closes connection in finally block"
  artifacts:
    - path: "workers/data-sync/src/api-clients/rate-limiter.ts"
      provides: "fetchWithBackoff and fetchWithDelay for rate-limited API calls"
      contains: "fetchWithBackoff"
    - path: "workers/data-sync/src/api-clients/fat-client.ts"
      provides: "createFatFetchPage returning FetchPageFn for Find a Tender"
      contains: "createFatFetchPage"
    - path: "workers/data-sync/src/api-clients/cf-client.ts"
      provides: "createCfFetchPage returning FetchPageFn for Contracts Finder"
      contains: "createCfFetchPage"
    - path: "workers/data-sync/src/index.ts"
      provides: "Worker scheduled handler wiring both API clients to sync engine"
      contains: "processSyncJob"
  key_links:
    - from: "workers/data-sync/src/index.ts"
      to: "workers/data-sync/src/sync-engine.ts"
      via: "processSyncJob calls for FaT and CF"
      pattern: "processSyncJob.*FIND_A_TENDER|processSyncJob.*CONTRACTS_FINDER"
    - from: "workers/data-sync/src/api-clients/fat-client.ts"
      to: "workers/data-sync/src/api-clients/rate-limiter.ts"
      via: "fetchWithDelay for rate-limited requests"
      pattern: "fetchWithDelay"
    - from: "workers/data-sync/src/api-clients/cf-client.ts"
      to: "workers/data-sync/src/api-clients/rate-limiter.ts"
      via: "fetchWithDelay for rate-limited requests"
      pattern: "fetchWithDelay"
    - from: "workers/data-sync/src/index.ts"
      to: "workers/data-sync/src/db/client.ts"
      via: "getDb/closeDb for MongoDB connection lifecycle"
      pattern: "getDb|closeDb"
---

<objective>
Implement the API clients for both Find a Tender and Contracts Finder OCDS APIs with rate limiting and exponential backoff, then wire the complete Worker scheduled handler that processes both sources sequentially in each hourly cron invocation.

Purpose: Complete the live data pipeline by connecting the sync engine (from Plan 10-01) to real API data sources and the Cloudflare Worker cron trigger.
Output: A fully functional Worker that can be deployed with `wrangler deploy` to start syncing UK procurement data hourly.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-live-data-pipeline/10-RESEARCH.md
@.planning/phases/10-live-data-pipeline/10-01-SUMMARY.md
@.planning/phases/02-data-pipeline/02-01-SUMMARY.md
@.planning/phases/02-data-pipeline/02-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement rate limiter and FaT + CF API clients returning FetchPageFn</name>
  <files>
    workers/data-sync/src/api-clients/rate-limiter.ts
    workers/data-sync/src/api-clients/fat-client.ts
    workers/data-sync/src/api-clients/cf-client.ts
  </files>
  <action>
    **src/api-clients/rate-limiter.ts** -- Shared rate limiting utilities:

    ```typescript
    const sleep = (ms: number) => new Promise((r) => setTimeout(r, ms));

    export async function fetchWithBackoff(url: string, maxRetries = 5): Promise<Response> {
      for (let attempt = 0; attempt < maxRetries; attempt++) {
        const res = await fetch(url);
        if (res.ok) return res;

        if (res.status === 429 || res.status === 403 || res.status === 503) {
          const retryAfter = parseInt(res.headers.get("Retry-After") ?? "0", 10);
          const backoff = retryAfter > 0
            ? retryAfter * 1000
            : Math.min(10000 * Math.pow(2, attempt), 300000);
          console.warn(
            `Rate limited (${res.status}). Attempt ${attempt + 1}/${maxRetries}. Waiting ${backoff / 1000}s...`
          );
          await sleep(backoff);
          continue;
        }

        const body = await res.text().catch(() => "(unreadable)");
        throw new Error(`HTTP ${res.status}: ${body.slice(0, 200)}`);
      }
      throw new Error(`Failed after ${maxRetries} retries`);
    }

    // ~6 req/min = 10s between requests
    export async function fetchWithDelay(url: string, delayMs = 10000): Promise<Response> {
      const res = await fetchWithBackoff(url);
      await sleep(delayMs);
      return res;
    }
    ```

    **src/api-clients/fat-client.ts** -- Find a Tender OCDS API client:

    Import `FetchPageFn` and `FetchPageResult` from `../sync-engine`, `OcdsRelease` from `../types`, and `fetchWithDelay` from `./rate-limiter`.

    Implement `createFatFetchPage(backfillStartDate: string): FetchPageFn`:
    - Returns a function matching the `FetchPageFn` signature
    - **CRITICAL per Phase 2 decision:** FaT API rejects comma-separated stages. Fetch tender and award stages SEPARATELY.
    - **CRITICAL per Phase 2 decision:** FaT API uses `links.next` as full URLs for pagination (not bare cursor tokens).
    - FaT API base URL: `https://www.find-tender.service.gov.uk/api/1.0/ocdsReleasePackages`
    - Parameters: `updatedFrom` (ISO 8601), `stage` (one of "tender" or "award"), `limit=100`

    Implementation logic:
    1. If `params.cursor` is provided and looks like a full URL (starts with "http"), follow it directly with `fetchWithDelay(cursor)`. This handles FaT's `links.next` continuation.
    2. If `params.cursor` is `"STAGE:award"` (a synthetic cursor we create -- see step 5), switch to fetching award stage from the beginning.
    3. If `params.cursor` is null (first call): build URL with `updatedFrom=backfillStartDate` (or `params.dateFrom` for delta sync), `stage=tender`, `limit=100`.
    4. Parse response JSON. Extract `releases` array from `data.releases`. Determine nextCursor:
       - If `data.links?.next` exists: nextCursor = that full URL
       - Else if we just finished tender stage: nextCursor = `"STAGE:award"` (synthetic cursor to trigger award stage fetch)
       - Else if we just finished award stage: nextCursor = null (both stages done)
    5. Track current stage internally via closure variable `currentStage: "tender" | "award"`.
    6. Return `{ releases, nextCursor }`.

    For delta sync (when `params.dateFrom` is provided instead of cursor):
    - Use `updatedFrom=dateFrom` parameter
    - Same dual-stage logic applies

    **src/api-clients/cf-client.ts** -- Contracts Finder OCDS API client:

    Import same types. Implement `createCfFetchPage(backfillStartDate: string): FetchPageFn`:
    - CF API base URL: `https://www.contractsfinder.service.gov.uk/Published/OCDS/Search`
    - **Per Phase 2 decision:** CF supports comma-separated stages: `stages=tender,award` in single request
    - **Per Phase 2 decision:** CF uses `publishedFrom`/`publishedTo` params (not updatedFrom)
    - **Per Phase 2 decision:** CF uses bare cursor tokens (not full URLs like FaT)
    - Parameters for backfill: `publishedFrom=backfillStartDate`, `stages=tender,award`, `limit=100`
    - Parameters for delta: `publishedFrom=dateFrom`, `stages=tender,award`, `limit=100`

    Implementation logic:
    1. If `params.cursor` is provided (bare token): build URL with cursor appended as query param
    2. If `params.cursor` is null: build URL from backfillStartDate or params.dateFrom
    3. Parse response JSON. Extract releases. Determine nextCursor from `data.cursor` or `data.nextCursor` (bare token) or `null` if no more pages.
    4. Return `{ releases, nextCursor }`.

    Both clients must handle empty `releases` arrays gracefully (return `{ releases: [], nextCursor: null }`).
  </action>
  <verify>
    1. `cd workers/data-sync && npx tsc --noEmit` -- compiles without errors
    2. `grep "createFatFetchPage" workers/data-sync/src/api-clients/fat-client.ts` -- factory function exported
    3. `grep "createCfFetchPage" workers/data-sync/src/api-clients/cf-client.ts` -- factory function exported
    4. `grep "STAGE:award" workers/data-sync/src/api-clients/fat-client.ts` -- synthetic stage cursor present
    5. `grep "fetchWithDelay" workers/data-sync/src/api-clients/rate-limiter.ts` -- rate limiter exported
    6. `grep "tender,award" workers/data-sync/src/api-clients/cf-client.ts` -- comma-separated stages for CF
  </verify>
  <done>
    Rate limiter with exponential backoff and 10s inter-request delay. FaT client handles dual-stage fetching (tender then award separately) with links.next full URL pagination and synthetic STAGE:award cursor for stage transition. CF client handles single-request dual-stage with bare cursor token pagination. Both return FetchPageFn compatible with the sync engine.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire Worker scheduled handler with full sync pipeline and verify compilation</name>
  <files>
    workers/data-sync/src/index.ts
  </files>
  <action>
    Replace the placeholder `src/index.ts` with the complete Worker entry point that wires API clients to the sync engine.

    ```typescript
    import type { Env } from "./types";
    import { getDb, closeDb } from "./db/client";
    import { processSyncJob } from "./sync-engine";
    import { createFatFetchPage } from "./api-clients/fat-client";
    import { createCfFetchPage } from "./api-clients/cf-client";
    ```

    Constants:
    - `FAT_BACKFILL_START = "2021-01-01T00:00:00Z"` -- FaT launch date, safe starting point for full backfill
    - `CF_BACKFILL_START = "2016-11-01T00:00:00Z"` -- CF OCDS data available from this date
    - `FAT_MAX_ITEMS = 5400` -- ~60% of per-invocation budget (90 pages * 60%)
    - `CF_MAX_ITEMS = 3600` -- ~40% of per-invocation budget (90 pages * 40%)

    Implement the scheduled handler:
    ```typescript
    export default {
      async scheduled(
        controller: ScheduledController,
        env: Env,
        ctx: ExecutionContext
      ): Promise<void> {
        const backfillStart = env.BACKFILL_START_DATE;
        const fatStart = backfillStart || FAT_BACKFILL_START;
        const cfStart = backfillStart || CF_BACKFILL_START;

        const db = await getDb(env.MONGODB_URI);

        try {
          // Process Find a Tender (allocate ~60% of budget)
          console.log("--- Starting Find a Tender sync ---");
          const fatFetchPage = createFatFetchPage(fatStart);
          const fatResult = await processSyncJob(
            db,
            "FIND_A_TENDER",
            fatFetchPage,
            fatStart,
            FAT_MAX_ITEMS
          );
          console.log(
            `FaT: fetched=${fatResult.fetched}, errors=${fatResult.errors}, done=${fatResult.done}`
          );

          // Process Contracts Finder (allocate ~40% of budget)
          console.log("--- Starting Contracts Finder sync ---");
          const cfFetchPage = createCfFetchPage(cfStart);
          const cfResult = await processSyncJob(
            db,
            "CONTRACTS_FINDER",
            cfFetchPage,
            cfStart,
            CF_MAX_ITEMS
          );
          console.log(
            `CF: fetched=${cfResult.fetched}, errors=${cfResult.errors}, done=${cfResult.done}`
          );

          console.log(
            `--- Sync complete: FaT=${fatResult.fetched}+CF=${cfResult.fetched} items, ` +
            `${fatResult.errors + cfResult.errors} errors ---`
          );
        } catch (err) {
          console.error("Sync failed:", err);
          throw err; // Let Cloudflare log the error
        } finally {
          await closeDb();
        }
      },
    } satisfies ExportedHandler<Env>;
    ```

    Key design decisions:
    - FaT processed first (higher priority, above-threshold UK government tenders)
    - CF processed second (below-threshold, higher volume but lower individual value)
    - `BACKFILL_START_DATE` env var optionally overrides both defaults (useful for limiting initial backfill to stay within M0 storage)
    - Always close DB connection in `finally` block (Worker isolate may not be reused)
    - Error thrown after logging so Cloudflare's cron retry mechanism can handle it
    - Both sources processed sequentially (not parallel) to respect combined rate limits

    After wiring, verify the complete project compiles:
    ```bash
    cd workers/data-sync && npx tsc --noEmit
    ```

    Also verify the Worker can be built by wrangler (does NOT deploy, just builds):
    ```bash
    cd workers/data-sync && npx wrangler deploy --dry-run --outdir dist
    ```
    This confirms wrangler can bundle all source files with the mongodb driver.
  </action>
  <verify>
    1. `cd workers/data-sync && npx tsc --noEmit` -- TypeScript compiles clean
    2. `cd workers/data-sync && npx wrangler deploy --dry-run --outdir dist 2>&1 | grep -v "error"` -- wrangler can bundle (ignore deploy-related warnings)
    3. `grep "processSyncJob" workers/data-sync/src/index.ts` -- sync engine wired
    4. `grep "FIND_A_TENDER" workers/data-sync/src/index.ts` -- FaT source configured
    5. `grep "CONTRACTS_FINDER" workers/data-sync/src/index.ts` -- CF source configured
    6. `grep "closeDb" workers/data-sync/src/index.ts` -- DB cleanup in finally
    7. `grep "BACKFILL_START_DATE" workers/data-sync/src/index.ts` -- configurable backfill start
  </verify>
  <done>
    Worker entry point wires FaT client (5400 items, 60% budget) and CF client (3600 items, 40% budget) to the sync engine, with MongoDB connection lifecycle managed in try/finally. Configurable BACKFILL_START_DATE env var for storage-constrained deployments. Project compiles with both TypeScript and wrangler bundler. Ready for deployment with `wrangler deploy` after setting MONGODB_URI secret.
  </done>
</task>

</tasks>

<verification>
1. `cd workers/data-sync && npx tsc --noEmit` -- full project compiles
2. `cd workers/data-sync && npx wrangler deploy --dry-run --outdir dist` -- wrangler can bundle
3. All API client files exist: `ls workers/data-sync/src/api-clients/{rate-limiter,fat-client,cf-client}.ts`
4. Worker entry point wires both sources: `grep -c "processSyncJob" workers/data-sync/src/index.ts` returns 2
5. Rate limiter has 10s delay: `grep "10000" workers/data-sync/src/api-clients/rate-limiter.ts`
6. FaT uses separate stages: `grep "STAGE:award" workers/data-sync/src/api-clients/fat-client.ts`
7. CF uses comma stages: `grep "tender,award" workers/data-sync/src/api-clients/cf-client.ts`
</verification>

<success_criteria>
- FaT API client handles dual-stage fetching with links.next URL pagination
- CF API client handles single-request comma-separated stages with bare cursor pagination
- Rate limiter enforces 10s delay (~6 req/min) with exponential backoff on 429/403/503
- Worker scheduled handler processes both sources sequentially with correct item budgets
- MongoDB connection opened at start, closed in finally block
- BACKFILL_START_DATE env var allows limiting historical depth
- Entire project compiles with TypeScript AND wrangler bundler
- Worker is deployable with `wrangler deploy` (after MONGODB_URI secret is set)
</success_criteria>

<output>
After completion, create `.planning/phases/10-live-data-pipeline/10-02-SUMMARY.md`
</output>
