---
phase: 11-invoice-spend-data-intelligence
plan: 02
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - workers/spend-ingest/src/stages/01-discover.ts
  - workers/spend-ingest/src/stages/02-extract-links.ts
  - workers/spend-ingest/src/db/buyers.ts
autonomous: true

must_haves:
  truths:
    - "Stage 1 discovers transparency/spending pages on buyer websites using direct fetch + Claude Haiku analysis"
    - "Stage 2 extracts CSV download URLs from discovered transparency pages"
    - "Both stages use cursor-based batching with crash-safe resume"
    - "Buyers without websites are skipped gracefully"
    - "Discovered transparency page URLs and CSV links are stored on the buyer document"
  artifacts:
    - path: "workers/spend-ingest/src/stages/01-discover.ts"
      provides: "Transparency page discovery stage"
      contains: "discoverTransparencyPages"
    - path: "workers/spend-ingest/src/stages/02-extract-links.ts"
      provides: "CSV link extraction stage"
      contains: "extractCsvLinks"
  key_links:
    - from: "workers/spend-ingest/src/stages/01-discover.ts"
      to: "workers/spend-ingest/src/db/buyers.ts"
      via: "getBuyerBatch for pagination"
      pattern: "getBuyerBatch"
    - from: "workers/spend-ingest/src/stages/01-discover.ts"
      to: "Claude Haiku API"
      via: "Anthropic SDK for page analysis"
      pattern: "messages.create"
    - from: "workers/spend-ingest/src/spend-engine.ts"
      to: "workers/spend-ingest/src/stages/01-discover.ts"
      via: "STAGE_FUNCTIONS registry"
      pattern: "discover.*discoverTransparencyPages"
---

<objective>
Implement the first two stages of the spend ingestion pipeline: AI-assisted discovery of transparency/spending pages on buyer websites, and extraction of CSV download URLs from those pages.

Purpose: These stages find where each buyer publishes their spending data. Without discovering transparency pages and CSV links, no spend data can be downloaded or parsed. This is the data discovery layer.

Output: Two working pipeline stages that crawl buyer websites, use Claude Haiku to identify transparency pages, and extract direct CSV download links.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/11-invoice-spend-data-intelligence/11-01-SUMMARY.md
@workers/enrichment/src/stages/01-classify.ts
@workers/enrichment/src/stages/01b-website-discovery.ts
@workers/enrichment/src/stages/05-personnel.ts
@workers/enrichment/src/api-clients/rate-limiter.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Stage 1 - Transparency page discovery</name>
  <files>
    workers/spend-ingest/src/stages/01-discover.ts
    workers/spend-ingest/src/db/buyers.ts
    workers/spend-ingest/src/spend-engine.ts
  </files>
  <action>
    Create `workers/spend-ingest/src/stages/01-discover.ts` implementing the `discover` stage:

    **Algorithm:**
    1. Fetch a batch of buyers that have a website but no transparencyPageUrl yet (use cursor-based $gt on _id, filter: { website: { $ne: null, $exists: true }, transparencyPageUrl: { $exists: false } })
    2. For each buyer, use the rate limiter to fetch the buyer's website homepage (10s AbortController timeout)
    3. If fetch succeeds, send the HTML content (first 12000 chars, stripped of scripts/styles) to Claude Haiku with a prompt asking it to find transparency/spending data pages
    4. Claude Haiku prompt should ask for: (a) URL of transparency or spending page, (b) any direct links to CSV files for "payments over 500" or similar, (c) confidence level
    5. Parse Claude's JSON response and update the buyer document with `transparencyPageUrl` and `csvLinks` array
    6. If fetch fails or Claude finds nothing, mark the buyer with `transparencyPageUrl: "none"` to skip in future runs
    7. Use p-limit(3) for concurrent processing within the batch
    8. Update job progress after each batch

    **Claude Haiku prompt (exact):**
    ```
    You are analyzing a UK public sector website to find spending transparency data.

    Organization: {buyerName}
    Website: {websiteUrl}
    Organization type: {orgType}

    HTML content:
    <html>{htmlContent}</html>

    Find links to:
    1. "transparency", "spending", "payments over 500", "expenditure", "open data" pages
    2. Direct CSV/Excel file download links containing spending/payment data
    3. Links to external open data portals (data.gov.uk, etc.)

    Look in navigation menus, footer links, and body content. UK councils typically have these under "Your Council" > "Transparency" or "About Us" > "Spending".

    Return ONLY valid JSON (no markdown):
    {
      "transparencyUrl": "full URL or null if not found",
      "csvLinks": ["array of direct CSV/XLS download URLs found, empty if none"],
      "confidence": "HIGH|MEDIUM|LOW|NONE"
    }
    ```

    **Rate limiting:** Use fetchWithDomainDelay from rate-limiter.ts. Default 2s delay between requests to the same domain.

    **HTML stripping:** Remove <script>, <style>, <noscript> tags and their contents using regex (same approach as workers/enrichment/src/stages/04-scrape.ts). Keep <a> tags with href attributes since those contain the links Claude needs to find.

    **Error handling:**
    - Network timeout: log warning, set transparencyPageUrl to "none"
    - Claude API error: log error, skip buyer, increment error count
    - Invalid JSON from Claude: log warning, set transparencyPageUrl to "none"

    Update `db/buyers.ts` to add:
    - `getBuyerBatchForDiscovery(db, cursor, limit)`: filter for buyers with website but no transparencyPageUrl
    - `updateBuyerTransparencyInfo(db, buyerId, { transparencyPageUrl, csvLinks })`: $set on buyer doc

    The engine already imports `discoverTransparencyPages` from this file (set up in Plan 01). Just replace the stub implementation with the real one.
  </action>
  <verify>Run `cd workers/spend-ingest && npx tsc --noEmit` to confirm compilation. Review that the stage function signature matches StageFn type.</verify>
  <done>Stage 1 fetches buyer websites, strips HTML, sends to Claude Haiku, parses JSON response, and stores transparency page URL and CSV links on buyer documents. Handles errors gracefully (timeout, API errors, invalid JSON). Uses cursor-based batching for resume.</done>
</task>

<task type="auto">
  <name>Task 2: Implement Stage 2 - CSV link extraction from transparency pages</name>
  <files>
    workers/spend-ingest/src/stages/02-extract-links.ts
    workers/spend-ingest/src/spend-engine.ts
  </files>
  <action>
    Create `workers/spend-ingest/src/stages/02-extract-links.ts` implementing the `extract_links` stage:

    **Purpose:** For buyers where Stage 1 found a transparencyPageUrl (not "none"), visit that page and extract ALL CSV download links. Stage 1 may have found some csvLinks from the homepage, but the transparency page typically has many more (monthly files spanning years).

    **Algorithm:**
    1. Fetch a batch of buyers with transparencyPageUrl set (not "none" and not null) that haven't been fully link-extracted yet (add field `csvLinksExtracted: Boolean` to track)
    2. For each buyer, fetch the transparency page HTML with rate limiting (10s timeout)
    3. Two-pass link extraction:
       a. **Regex pass**: Find all href attributes ending in .csv, .CSV, .xls, .xlsx, .XLS, .XLSX. Also match hrefs containing "download" or "export" with "csv" in the URL or nearby text.
       b. **Claude Haiku pass** (if regex found < 3 links): Send the HTML (first 15000 chars) to Claude Haiku asking it to find ALL CSV/Excel download links for spending data. This catches JavaScript-generated links, data portals with non-obvious URLs, and links behind redirects.
    4. Normalize found URLs: resolve relative URLs against the transparency page URL base, deduplicate, filter out non-http(s) URLs
    5. Merge with any csvLinks from Stage 1 (deduplicate)
    6. Update buyer document: $set csvLinks array, $set csvLinksExtracted: true
    7. Track stats: total links found across all buyers

    **Claude Haiku prompt for link extraction (only used when regex found < 3 links):**
    ```
    You are extracting CSV/Excel download links from a UK public sector transparency page.

    Organization: {buyerName}
    Page URL: {transparencyPageUrl}

    HTML:
    <html>{htmlContent}</html>

    Find ALL download links for spending/payment CSV or Excel files. These are typically monthly or quarterly reports titled like "Payments over 500 - January 2024" or "Expenditure 2023-24 Q1".

    Return ONLY valid JSON:
    {
      "csvLinks": ["array of full download URLs"]
    }
    ```

    **URL normalization:** Use `new URL(href, baseUrl)` to resolve relative URLs. Filter: must start with http:// or https://, must end in .csv/.xls/.xlsx (case insensitive) OR contain "download"/"export" in the path.

    The engine already imports `extractCsvLinks` from this file (set up in Plan 01). Just replace the stub implementation with the real one.
  </action>
  <verify>Run `cd workers/spend-ingest && npx tsc --noEmit` to confirm compilation. Verify the stage handles the case where transparencyPageUrl fetch fails (skip, don't crash).</verify>
  <done>Stage 2 visits transparency pages, extracts CSV download links via regex + optional Claude Haiku fallback, normalizes URLs, and stores the complete csvLinks array on buyer documents. Sets csvLinksExtracted flag for resume tracking.</done>
</task>

</tasks>

<verification>
1. `cd workers/spend-ingest && npx tsc --noEmit` passes
2. Stage 1 function exported as `discoverTransparencyPages` (engine imports it from Plan 01)
3. Stage 2 function exported as `extractCsvLinks` (engine imports it from Plan 01)
4. Both stages use getBuyerBatch-style cursor pagination
5. Both stages use rate limiter for HTTP requests
6. Both stages use p-limit for concurrency control
7. Claude Haiku prompts are complete and request JSON-only output
</verification>

<success_criteria>
Both pipeline stages compile, are wired into the engine, and implement the full discovery flow: website fetch -> HTML strip -> Claude Haiku analysis -> transparency URL storage -> page fetch -> CSV link extraction -> csvLinks array stored on buyer.
</success_criteria>

<output>
After completion, create `.planning/phases/11-invoice-spend-data-intelligence/11-02-SUMMARY.md`
</output>
