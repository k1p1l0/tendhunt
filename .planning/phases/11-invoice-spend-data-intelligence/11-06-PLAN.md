---
phase: 11-invoice-spend-data-intelligence
plan: 06
type: execute
wave: 3
depends_on: ["11-02", "11-03"]
files_modified:
  - workers/spend-ingest/src/stages/01-discover.ts
  - workers/spend-ingest/src/stages/02-extract-links.ts
  - workers/spend-ingest/src/stages/03-download-parse.ts
  - workers/spend-ingest/src/normalization/known-schemas.ts
  - workers/spend-ingest/src/normalization/column-mapper.ts
  - workers/spend-ingest/package.json
  - web/scripts/test-spend-single.ts
autonomous: true

must_haves:
  truths:
    - "Stage 1 filters GOV.UK collection page links to the current buyer's department using slugified name keywords and abbreviation map"
    - "Stage 2 follows GOV.UK publication pages to resolve actual download URLs (ODS/CSV) from assets.publishing.service.gov.uk"
    - "Stage 3 parses ODS and XLSX files via SheetJS xlsx library alongside existing CSV/PapaParse parsing"
    - "GOV.UK spending-over-25k schema is recognized by known-schemas.ts without AI fallback"
    - "End-to-end pipeline works for MoD: discover → filter → follow publications → parse ODS → normalize → aggregate"
  artifacts:
    - path: "workers/spend-ingest/src/stages/01-discover.ts"
      provides: "Buyer-specific link filtering for GOV.UK collection pages"
      contains: "filterLinksToBuyer"
    - path: "workers/spend-ingest/src/stages/02-extract-links.ts"
      provides: "GOV.UK publication page resolver"
      contains: "followGovukPublicationPages"
    - path: "workers/spend-ingest/src/stages/03-download-parse.ts"
      provides: "ODS/XLSX spreadsheet parsing via SheetJS"
      contains: "parseSpreadsheet"
    - path: "workers/spend-ingest/src/normalization/known-schemas.ts"
      provides: "GOV.UK spending-over-25k column mapping"
      contains: "govuk_spending_25k"
  key_links:
    - from: "workers/spend-ingest/src/stages/02-extract-links.ts"
      to: "workers/spend-ingest/src/api-clients/rate-limiter.ts"
      via: "fetchWithDomainDelay for gov.uk rate limiting"
      pattern: "fetchWithDomainDelay"
    - from: "workers/spend-ingest/src/stages/03-download-parse.ts"
      to: "xlsx"
      via: "SheetJS for ODS/XLSX parsing"
      pattern: "XLSX.read"
---

<objective>
Extend the spend-ingest pipeline to handle GOV.UK central government buyers (MoD, HMRC, etc.) which use a two-level page structure and ODS file format.

Purpose: Plans 11-02/03 built the pipeline for local councils (direct CSV links). Central government buyers on GOV.UK use collection pages → publication pages → ODS downloads, which the current pipeline can't follow. MoD specifically uses ODS format which Stage 3 can't parse.

Output: Full end-to-end pipeline for MoD: pattern match → filter to buyer's publications → follow to download URLs → parse ODS → normalize via known schema → aggregate.
</objective>

<execution_context>
GOV.UK two-level structure:
- Level 1: /government/collections/spending-over-25-000 → lists ALL departments' publication pages (190 links for all depts)
- Level 2: /government/publications/mod-spending-over-25000-... → contains actual ODS/CSV download URLs

MoD ODS columns: Department family, Entity, Date, Expense Type, Expense Area, Supplier, Transaction Number, Amount, Description

Key constraint: Stage 1 already has buyer object in scope. Stage 2 runs before regex extraction. Stage 3 content-type check needs broadening.
</execution_context>

<tasks>

<task id="1" title="Filter Stage 1 links to buyer's department">
<description>
Add filterLinksToBuyer(links, buyerName) to 01-discover.ts. GOV.UK collection pages list ALL departments — we need to keep only the current buyer's links.

Implementation:
1. Add DEPT_ABBREVIATION_MAP constant mapping department name patterns to URL slugs:
   - "Ministry of Defence" → ["mod", "ministry-of-defence", "defence"]
   - "HM Revenue" → ["hmrc", "hm-revenue", "revenue-customs"]
   - Cover ~20 major central government departments
2. filterLinksToBuyer function:
   - Slugify buyer name into keywords
   - Split into significant words (skip "the", "of", "and", etc.)
   - Look up abbreviation map for matching patterns
   - Filter links where URL contains any keyword
   - Fallback: if zero match, return all links
3. Call filterLinksToBuyer after extractCsvLinksFromHtml when URL is gov.uk/government/
</description>
<file>workers/spend-ingest/src/stages/01-discover.ts</file>
</task>

<task id="2" title="Follow GOV.UK publication pages in Stage 2">
<description>
Add followGovukPublicationPages() to 02-extract-links.ts. Runs BEFORE extractLinksEnhanced() to resolve publication HTML pages to actual download URLs.

Implementation:
1. followGovukPublicationPages(csvLinks, buyerName) function:
   - Identify publication URLs: contains /government/publications/
   - For each (max 24 per buyer), fetch HTML via fetchWithDomainDelay
   - Extract download URLs matching:
     - assets.publishing.service.gov.uk/media/ (ODS/CSV downloads)
     - /government/uploads/ pattern
   - Extract href values from anchor tags, resolve to absolute URLs
   - Replace each publication URL with its resolved download URL(s)
   - Return updated csvLinks array
2. Insert call at start of per-buyer processing loop (before extractLinksEnhanced)
3. Also filter resolved links to buyer using same keyword approach as Stage 1
4. Sequential processing within a buyer (same gov.uk domain, rate-limited)
</description>
<file>workers/spend-ingest/src/stages/02-extract-links.ts</file>
</task>

<task id="3" title="Add ODS/XLSX parsing to Stage 3">
<description>
Install xlsx (SheetJS) dependency and modify 03-download-parse.ts to handle ODS/XLSX files alongside CSV.

Implementation:
1. Add "xlsx": "^0.18.5" to spend-ingest package.json dependencies
2. Add parseSpreadsheet(buffer: ArrayBuffer) helper:
   - XLSX.read(buffer, { type: "array" })
   - Get first sheet: wb.Sheets[wb.SheetNames[0]]
   - XLSX.utils.sheet_to_json<Record<string, string>>(sheet, { raw: false })
   - Extract headers from first row
   - Return { data, headers } matching PapaParse output shape
3. Add detectFileFormat(contentType, url) helper:
   - CSV: text/csv, text/plain, application/csv
   - ODS: application/vnd.oasis.opendocument.spreadsheet, .ods extension
   - XLSX: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet, .xlsx extension
   - application/octet-stream → check URL extension fallback
4. Modify download loop:
   - Broaden content-type acceptance (any vnd.*, octet-stream, csv, text)
   - Branch on format: CSV → response.text() + PapaParse (existing), ODS/XLSX → response.arrayBuffer() + parseSpreadsheet
   - Both paths produce same { data, headers } shape for downstream normalization
</description>
<file>workers/spend-ingest/src/stages/03-download-parse.ts</file>
<file>workers/spend-ingest/package.json</file>
</task>

<task id="4" title="Add GOV.UK spending schema + fix model ID">
<description>
1. Add govuk_spending_25k schema to KNOWN_SCHEMAS in known-schemas.ts:
   - detect: hasHeaders(headers, ["expense type", "expense area", "supplier", "transaction number"])
   - Map: date→Date, amount→Amount, vendor→Supplier, category→Expense Area, subcategory→Expense Type, department→Entity, reference→Transaction Number
   - Insert FIRST in array (most specific — requires "transaction number" which Devon files don't have)
2. Fix Claude model ID in column-mapper.ts:
   - Line 87: "claude-haiku-4-5-20250401" → "claude-haiku-4-5-20251001"
</description>
<file>workers/spend-ingest/src/normalization/known-schemas.ts</file>
<file>workers/spend-ingest/src/normalization/column-mapper.ts</file>
</task>

<task id="5" title="Update test script for GOV.UK + ODS">
<description>
Update test-spend-single.ts to exercise the full GOV.UK pipeline:

1. Stage 1: Add buyer-specific link filtering (same filterLinksToBuyer logic, inlined)
2. Stage 2: Add GOV.UK publication page following:
   - Detect /government/publications/ URLs in csvLinks
   - Fetch each, extract download URLs from assets.publishing.service.gov.uk
   - Replace publication URLs with download URLs
3. Stage 3: Add ODS/XLSX parsing:
   - Add xlsx import (install as devDependency in apps/web)
   - Detect format from content-type/URL extension
   - ODS/XLSX: arrayBuffer → XLSX.read → sheet_to_json
   - CSV: existing PapaParse flow
4. Limit to 3 most recent files in test mode (sort by URL date pattern)
5. Use known schema matching before heuristic/AI column mapping
</description>
<file>web/scripts/test-spend-single.ts</file>
</task>

</tasks>

<verification>
1. TypeScript: `cd apps/workers/spend-ingest && npx tsc --noEmit` passes
2. Run test: `cd apps/web && DOTENV_CONFIG_PATH=.env.local npx tsx --require dotenv/config scripts/test-spend-single.ts "Ministry of Defence"`
3. Expected:
   - Stage 1: Pattern match on first probe, ~12-24 MoD-only publication links (not 190)
   - Stage 2: Publication pages followed → ODS download URLs from assets.publishing.service.gov.uk
   - Stage 3: ODS files parsed via xlsx, normalized via govuk_spending_25k schema
   - Stage 4: SpendSummary created with category/vendor/monthly breakdowns
4. MongoDB: spendtransactions collection has MoD records, spendsummaries has aggregate
</verification>
