---
phase: 11-invoice-spend-data-intelligence
plan: 03
type: execute
wave: 2
depends_on: ["11-01"]
files_modified:
  - workers/spend-ingest/src/stages/03-download-parse.ts
  - workers/spend-ingest/src/stages/04-aggregate.ts
  - workers/spend-ingest/src/normalization/column-mapper.ts
  - workers/spend-ingest/src/normalization/known-schemas.ts
  - workers/spend-ingest/src/normalization/category-taxonomy.ts
  - workers/spend-ingest/src/normalization/date-parser.ts
  - workers/spend-ingest/src/normalization/amount-parser.ts
  - workers/spend-ingest/src/normalization/vendor-normalizer.ts
autonomous: true

must_haves:
  truths:
    - "Stage 3 downloads CSV files, parses them with PapaParse, normalizes columns via hybrid pattern library + AI fallback, and bulk-upserts SpendTransaction documents"
    - "Stage 4 computes per-buyer SpendSummary aggregates (category breakdown, vendor breakdown, monthly totals, date range) from stored transactions"
    - "CSV column normalization handles at least 5 known UK council formats (Devon, Rochdale, Ipswich, Manchester, Eden patterns)"
    - "Amount parsing handles GBP symbols, parentheses-negative, comma separators, and CR/DR prefixes"
    - "Date parsing handles ISO 8601, DD/MM/YYYY, DD-Mon-YY, and DD-Mon-YYYY formats"
    - "Vendor names are normalized (lowercase, strip Ltd/Limited/PLC, trim) for consistent dedup"
    - "Storage-safe: caps at MAX_TRANSACTIONS_PER_BUYER (configurable, default 5000) to respect MongoDB Atlas limits"
  artifacts:
    - path: "workers/spend-ingest/src/stages/03-download-parse.ts"
      provides: "CSV download, parse, and normalize stage"
      contains: "downloadAndParseCsvs"
    - path: "workers/spend-ingest/src/stages/04-aggregate.ts"
      provides: "Spend summary aggregation stage"
      contains: "aggregateSpendData"
    - path: "workers/spend-ingest/src/normalization/column-mapper.ts"
      provides: "Hybrid column normalization (pattern library + AI fallback)"
      contains: "mapColumns"
    - path: "workers/spend-ingest/src/normalization/known-schemas.ts"
      provides: "10 known UK council CSV column mappings"
      contains: "KNOWN_SCHEMAS"
    - path: "workers/spend-ingest/src/normalization/category-taxonomy.ts"
      provides: "Spend category normalization to ~25 high-level categories"
      contains: "normalizeCategory"
  key_links:
    - from: "workers/spend-ingest/src/stages/03-download-parse.ts"
      to: "workers/spend-ingest/src/normalization/column-mapper.ts"
      via: "mapColumns called for each CSV file"
      pattern: "mapColumns"
    - from: "workers/spend-ingest/src/stages/03-download-parse.ts"
      to: "workers/spend-ingest/src/db/spend-data.ts"
      via: "bulkUpsertTransactions for batch insert"
      pattern: "bulkUpsertTransactions"
    - from: "workers/spend-ingest/src/stages/04-aggregate.ts"
      to: "workers/spend-ingest/src/db/spend-data.ts"
      via: "MongoDB aggregation pipeline + upsertSpendSummary"
      pattern: "upsertSpendSummary"
---

<objective>
Implement Stage 3 (CSV download, parsing, and normalization) and Stage 4 (spend summary aggregation), plus the full normalization library for column mapping, date/amount parsing, vendor name normalization, and category taxonomy.

Purpose: This is the data processing core of the pipeline. Stage 3 turns raw CSV files from hundreds of different council formats into normalized SpendTransaction records. Stage 4 pre-computes aggregates so the frontend can render charts instantly without scanning millions of rows.

Output: Two working pipeline stages plus a complete normalization library covering column mapping, date parsing, amount parsing, vendor normalization, and category taxonomy.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/11-invoice-spend-data-intelligence/11-01-SUMMARY.md
@.planning/phases/11-invoice-spend-data-intelligence/11-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build CSV normalization library (column mapper, parsers, taxonomy)</name>
  <files>
    workers/spend-ingest/src/normalization/column-mapper.ts
    workers/spend-ingest/src/normalization/known-schemas.ts
    workers/spend-ingest/src/normalization/category-taxonomy.ts
    workers/spend-ingest/src/normalization/date-parser.ts
    workers/spend-ingest/src/normalization/amount-parser.ts
    workers/spend-ingest/src/normalization/vendor-normalizer.ts
  </files>
  <action>
    **known-schemas.ts**: Define ~10 known UK council CSV column mapping patterns as an array of ColumnMapping objects:
    ```typescript
    interface ColumnMapping {
      name: string;  // e.g., "devon_pattern"
      detect: (headers: string[]) => boolean;
      map: { date: string; amount: string; vendor: string; category: string; subcategory?: string; department?: string; reference?: string; };
    }
    ```
    Include at minimum these 5 verified patterns from research:
    1. Devon: Expense Area, Expense Type, Amount, Supplier Name, Date
    2. Rochdale: DIRECTORATE, PURPOSE, AMOUNT (GBP), SUPPLIER NAME, EFFECTIVE DATE
    3. Ipswich: Service Area Categorisation, Expenses Type, Amount, Supplier Name, Date
    4. Manchester: Service Area, Net Amount, Supplier Name, Invoice Payment Date
    5. Eden: Expense Area, Amount, Supplier Name, Date, Body Name
    Plus 5 more common variations:
    6. Generic1: Department, Category, Total (inc. VAT), Payee Name, Payment Date
    7. Generic2: Service, Description, Value, Supplier, Date Paid
    8. Generic3: Cost Centre, Subjective, Net, Creditor Name, Posting Date
    9. NHS pattern: Budget Code, Description, Net Amount, Supplier, Invoice Date
    10. ProClass: ProClass Description, Total, Supplier Name, Date

    Detection functions should be case-insensitive and check for 2-3 key distinguishing columns.

    **column-mapper.ts**: Export `mapColumns(headers: string[], sampleRows: string[][], anthropicApiKey: string): Promise<ColumnMapping["map"] | null>`:
    1. Try each known schema's detect() function against headers
    2. If a known schema matches, return its map immediately (no AI call)
    3. If no match, call Claude Haiku with the headers and first 3 sample rows, asking it to map to our unified schema fields (date, amount, vendor, category, subcategory, department, reference)
    4. Cache the AI-generated mapping in memory (Map<string, ColumnMapping["map"]> keyed by sorted headers hash) so repeat calls for the same schema don't re-invoke AI
    5. Return null if even AI can't map (e.g., completely unrelated CSV)

    Claude Haiku column mapping prompt:
    ```
    Map these CSV columns to a unified spending data schema.

    CSV headers: {headers.join(", ")}
    Sample rows:
    {sampleRows.map(r => r.join(" | ")).join("\n")}

    Map to these fields (use the exact CSV column name, or null if not found):
    - date: column containing the payment/transaction date
    - amount: column containing the payment amount in GBP
    - vendor: column containing the supplier/payee name
    - category: column containing the spend category, service area, or purpose
    - subcategory: column with more specific categorization (or null)
    - department: column with the council department or directorate (or null)
    - reference: column with transaction/invoice number (or null)

    Return ONLY valid JSON:
    { "date": "column_name", "amount": "column_name", "vendor": "column_name", "category": "column_name", "subcategory": null, "department": null, "reference": null }
    ```

    **date-parser.ts**: Export `parseFlexibleDate(raw: string): Date | null` that tries formats in order:
    1. ISO 8601 (YYYY-MM-DD or YYYY-MM-DDTHH:mm:ss)
    2. DD/MM/YYYY or DD-MM-YYYY
    3. DD-Mon-YY and DD-Mon-YYYY (e.g., "12-Nov-25", "12-November-2025")
    4. MM/DD/YYYY (fallback, only if day > 12 to disambiguate)
    5. Return null if none match
    Two-digit year rule: < 50 = 2000s, >= 50 = 1900s.

    **amount-parser.ts**: Export `parseAmount(raw: string): number` that handles:
    - Parentheses as negative: "(1,234.56)" -> -1234.56
    - CR/DR prefix: "CR 500.00" -> -500.00
    - Currency symbols: strip GBP, $, etc.
    - Comma thousands separators: "1,234,567.89" -> 1234567.89
    - Negative sign: "-1234.56" -> -1234.56
    - Return 0 for unparseable values

    **vendor-normalizer.ts**: Export `normalizeVendor(raw: string): string` that:
    1. Trim and lowercase
    2. Strip common suffixes: " ltd", " limited", " plc", " inc", " llp", " llc", " uk", " (uk)", " co", " corp", " corporation", " group"
    3. Strip punctuation (periods, commas, dashes at end)
    4. Collapse multiple spaces to single space
    5. Return the normalized string (used as vendorNormalized key for dedup)

    **category-taxonomy.ts**: Export `normalizeCategory(raw: string): string` with ~25 high-level categories:
    - IT & Digital, Professional Services, Facilities & Maintenance, Construction & Capital Works, Transport & Fleet, Healthcare & Social Care, Education & Training, Energy & Utilities, Waste Management, Legal Services, Financial Services, HR & Recruitment, Communications & Marketing, Environmental Services, Housing, Planning & Development, Cultural & Leisure, Emergency Services, Catering & Hospitality, Office Supplies & Equipment, Insurance, Grants & Subsidies, Property & Estates, Consultancy, Other
    - Use keyword matching: map of keywords to categories (e.g., "IT" | "computer" | "software" | "digital" | "technology" | "cyber" -> "IT & Digital")
    - Case-insensitive matching on the raw category string
    - Return "Other" if no keyword matches
  </action>
  <verify>Run `cd workers/spend-ingest && npx tsc --noEmit` to verify compilation. Manually review that the 5 verified CSV formats (Devon, Rochdale, Ipswich, Manchester, Eden) are correctly mapped.</verify>
  <done>Complete normalization library with 10 known schemas, hybrid column mapper with AI fallback, robust date/amount parsers, vendor normalizer, and 25-category taxonomy. All functions are pure (no side effects except AI call in column-mapper) and handle edge cases.</done>
</task>

<task type="auto">
  <name>Task 2: Implement Stage 3 (CSV download/parse) and Stage 4 (aggregation)</name>
  <files>
    workers/spend-ingest/src/stages/03-download-parse.ts
    workers/spend-ingest/src/stages/04-aggregate.ts
    workers/spend-ingest/src/spend-engine.ts
  </files>
  <action>
    **Stage 3 — 03-download-parse.ts**: Export `downloadAndParseCsvs` matching StageFn signature:

    1. Fetch batch of buyers with csvLinks array that is non-empty and haven't been fully processed yet (add `spendDataIngested: boolean` field tracking). Use cursor-based $gt on _id.
    2. For each buyer, iterate over their csvLinks array:
       a. Check if this CSV URL was already processed (look in SpendTransaction collection for sourceFile matching this URL, or check buyer's csvFilesProcessed in SpendSummary). Skip if already done.
       b. Fetch the CSV file via rate limiter (15s timeout). If the response is not text/csv or text/plain content-type, skip (handles Excel files disguised as CSV).
       c. Parse with PapaParse: `Papa.parse(csvText, { header: true, skipEmptyLines: true, dynamicTyping: false })`. Use the `errors` array to track malformed rows.
       d. If parse.errors.length / parse.data.length > 0.5, log warning and skip file (>50% error rate).
       e. On first file for this buyer: call `mapColumns(headers, sampleRows, env.ANTHROPIC_API_KEY)` to get the column mapping. Cache this mapping for subsequent files from the same buyer (most councils use the same format across months).
       f. For each parsed row, normalize: parseFlexibleDate for date, parseAmount for amount, normalizeVendor for vendor, normalizeCategory for category. Skip rows where amount is 0 or date is null.
       g. Batch normalized rows into chunks of 500, call `bulkUpsertTransactions(db, chunk)`.
       h. **Storage cap**: Track transaction count per buyer. Stop ingesting for a buyer once MAX_TRANSACTIONS_PER_BUYER (default 5000) is reached. Log if cap was hit.
    3. After all CSVs for a buyer are processed, mark `spendDataIngested: true` on buyer.
    4. Update job progress with cursor, processed count, error count.

    Use p-limit(2) for concurrent buyer processing (not file processing — each buyer processes files sequentially to avoid memory spikes).

    **Stage 4 — 04-aggregate.ts**: Export `aggregateSpendData` matching StageFn signature:

    1. Fetch batch of buyers that have spendDataIngested: true but need aggregation (either no SpendSummary exists, or SpendSummary.lastComputedAt < buyer.lastSpendIngestAt).
    2. For each buyer, run a MongoDB aggregation pipeline on SpendTransaction:
       ```
       db.collection("spendtransactions").aggregate([
         { $match: { buyerId: buyerObjectId } },
         { $facet: {
           totals: [{ $group: { _id: null, totalSpend: { $sum: "$amount" }, totalTransactions: { $sum: 1 }, earliest: { $min: "$date" }, latest: { $max: "$date" } } }],
           byCategory: [{ $group: { _id: "$category", total: { $sum: "$amount" }, count: { $sum: 1 } } }, { $sort: { total: -1 } }, { $limit: 30 }],
           byVendor: [{ $group: { _id: "$vendorNormalized", vendor: { $first: "$vendor" }, total: { $sum: "$amount" }, count: { $sum: 1 } } }, { $sort: { total: -1 } }, { $limit: 50 }],
           byMonth: [{ $group: { _id: { year: { $year: "$date" }, month: { $month: "$date" } }, total: { $sum: "$amount" } } }, { $sort: { "_id.year": 1, "_id.month": 1 } }],
         }}
       ])
       ```
    3. Transform aggregation results into SpendSummary shape and call `upsertSpendSummary(db, buyerId, summary)`.
    4. Update buyer document with `spendDataAvailable: true` and `lastSpendIngestAt: new Date()`.
    5. Update job progress.

    The engine already imports `downloadAndParseCsvs` and `aggregateSpendData` from these files (set up in Plan 01). Just replace the stub implementations with the real ones.
  </action>
  <verify>Run `cd workers/spend-ingest && npx tsc --noEmit` to confirm compilation. Verify both stages are wired into STAGE_FUNCTIONS record. Check that Stage 3 has the MAX_TRANSACTIONS_PER_BUYER cap.</verify>
  <done>Stage 3 downloads CSVs, parses with PapaParse, normalizes via hybrid column mapper, bulk-upserts transactions (capped per buyer), and tracks processed files. Stage 4 runs MongoDB aggregation pipelines to compute SpendSummary documents with category/vendor/monthly breakdowns. Both stages use cursor-based batching and are wired into the pipeline engine.</done>
</task>

</tasks>

<verification>
1. `cd workers/spend-ingest && npx tsc --noEmit` passes
2. Stage functions exported with correct names matching engine imports from Plan 01
3. Known schemas array has 10 entries with detect/map functions
4. parseFlexibleDate handles all 4 date format groups
5. parseAmount handles parentheses, CR prefix, currency symbols, commas
6. normalizeCategory maps to ~25 categories via keyword matching
7. normalizeVendor strips suffixes and normalizes casing
8. Stage 3 has MAX_TRANSACTIONS_PER_BUYER cap
9. Stage 4 uses MongoDB $facet aggregation for efficient multi-metric computation
</verification>

<success_criteria>
The complete CSV processing pipeline compiles: download -> parse (PapaParse) -> normalize columns (hybrid mapper) -> normalize values (date/amount/vendor/category) -> bulk upsert transactions (capped) -> aggregate summaries. All normalization functions handle the documented UK council CSV edge cases.
</success_criteria>

<output>
After completion, create `.planning/phases/11-invoice-spend-data-intelligence/11-03-SUMMARY.md`
</output>
