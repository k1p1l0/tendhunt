---
phase: 05-vibe-scanner
plan: 04
type: execute
wave: 3
depends_on: ["05-01"]
files_modified:
  - src/app/api/scanners/[id]/score/route.ts
  - src/lib/scoring-engine.ts
autonomous: true

must_haves:
  truths:
    - "AI batch-scores entities using Claude Haiku 4.5 with prompt caching"
    - "Scoring uses SSE to stream progress events to the client in real-time"
    - "Each AI column is scored separately per entity"
    - "Prompt caching is enabled on the system prompt (>4096 tokens)"
    - "Scores are persisted to the Scanner document in MongoDB"
    - "Failed scoring on individual entities does not block the batch"
  artifacts:
    - path: "src/app/api/scanners/[id]/score/route.ts"
      provides: "POST SSE endpoint that batch-scores all entities for a scanner"
      exports: ["POST"]
    - path: "src/lib/scoring-engine.ts"
      provides: "Scoring engine with Claude Haiku API calls, prompt construction, and batch processing"
      exports: ["scoreEntities", "scoreOneEntity", "buildScoringSystemPrompt", "buildEntityUserPrompt"]
  key_links:
    - from: "src/app/api/scanners/[id]/score/route.ts"
      to: "src/lib/scoring-engine.ts"
      via: "scoreEntities function for batch processing"
      pattern: "scoreEntities|scoreOneEntity"
    - from: "src/lib/scoring-engine.ts"
      to: "src/lib/anthropic.ts"
      via: "Claude Haiku API calls with prompt caching"
      pattern: "anthropic\\.messages\\.create"
    - from: "src/app/api/scanners/[id]/score/route.ts"
      to: "src/models/scanner.ts"
      via: "Persist scores to Scanner.scores array"
      pattern: "Scanner\\.(updateOne|findOneAndUpdate)"
---

<objective>
Build the batch scoring SSE engine that scores all entities in a scanner across all AI columns using Claude Haiku with prompt caching and real-time progress streaming.

Purpose: This is the AI engine that powers every scanner. It must be efficient (prompt caching for cost), resilient (per-entity error handling), and responsive (SSE progress events). This plan is deliberately separate from the table UI to keep concerns clean.

Output: Scoring engine library and SSE API endpoint that can score any scanner type's entities across multiple AI columns.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-vibe-scanner/05-CONTEXT.md
@.planning/phases/05-vibe-scanner/05-RESEARCH.md
@.planning/phases/05-vibe-scanner/05-01-SUMMARY.md

# Key source files
@src/models/scanner.ts                    # Scanner model (from Plan 01)
@src/lib/scanners.ts                      # Scanner CRUD (from Plan 01)
@src/lib/anthropic.ts                    # Anthropic client singleton
@src/models/contract.ts                  # Contract schema
@src/models/signal.ts                    # Signal schema
@src/models/buyer.ts                     # Buyer schema
@src/app/api/vibe-scanner/score/route.ts # Old scoring endpoint (reference for SSE pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scoring engine library with prompt caching and batch processing</name>
  <files>src/lib/scoring-engine.ts</files>
  <action>
**Create scoring engine** at `src/lib/scoring-engine.ts`:

This is the core AI scoring logic, extracted from the old score route into a reusable library.

**1. buildScoringSystemPrompt function:**
```typescript
function buildScoringSystemPrompt(scanner: IScanner, columnPrompt: string): string
```
Combines:
- The scanner's base scoringPrompt (company profile context, >4096 tokens for caching)
- The specific AI column's prompt template
- Instructions for response format

The system prompt must exceed 4096 tokens for Claude Haiku 4.5 prompt caching. The scanner.scoringPrompt already does this (from Plan 01 which reuses the old generateScoringPrompt with CPV codes, regions, terminology padding).

Structure:
```
[scanner.scoringPrompt -- company profile + UK procurement context, >4096 tokens]

---

## Current Analysis Task

[columnPrompt -- the specific AI column's evaluation instructions]

Respond with valid JSON:
{
  "score": <number 1.0-10.0 or null if not applicable>,
  "reasoning": "<1-2 sentence explanation>",
  "response": "<full analysis text>"
}
```

The `score` field can be null for columns that produce text responses (like "Key Contact" for buyers scanner). The `response` field contains the full text answer.

**2. buildEntityUserPrompt function:**
```typescript
function buildEntityUserPrompt(entity: Record<string, unknown>, scannerType: ScannerType, searchQuery?: string): string
```
Constructs the user message from entity data depending on scanner type:

For RFP:
```
Analyze this contract:

Title: {title}
Buyer: {buyerName}
Description: {description, truncated to 2000 chars}
Sector: {sector}
Region: {buyerRegion}
Value: {valueMin}-{valueMax} GBP
CPV Codes: {cpvCodes joined}
Deadline: {deadlineDate}

Search Query Context: {searchQuery}
```

For Meetings:
```
Analyze this board meeting signal:

Organization: {organizationName}
Signal Type: {signalType}
Title: {title}
Insight: {insight}
Sector: {sector}
Source Date: {sourceDate}

Search Query Context: {searchQuery}
```

For Buyers:
```
Analyze this buyer organization:

Organization: {name}
Sector: {sector}
Region: {region}
Description: {description, truncated to 1000 chars}
Contract Count: {contractCount}
Website: {website}

Search Query Context: {searchQuery}
```

Include the search query as context so the AI can evaluate relevance to the scanner's focus.

**3. scoreOneEntity function:**
```typescript
async function scoreOneEntity(
  entity: Record<string, unknown>,
  scannerType: ScannerType,
  systemPrompt: string,
  searchQuery?: string
): Promise<{ score: number | null; reasoning: string; response: string }>
```
- Calls `anthropic.messages.create()` with:
  - model: "claude-haiku-4-5-20251001"
  - max_tokens: 300
  - system: `[{ type: "text", text: systemPrompt, cache_control: { type: "ephemeral" } }]`
  - messages: user message from buildEntityUserPrompt
  - output_config with json_schema: `{ score: { type: "number", nullable: true }, reasoning: { type: "string" }, response: { type: "string" } }`
- Parse the JSON response
- Return the score, reasoning, and full response

Note: Use `"nullable": true` on the score property in the JSON schema so columns that produce text (not numeric) can return null. If the SDK doesn't support nullable in json_schema, use `type: ["number", "null"]` or wrap in a try-catch and default score to null if not a number.

**4. scoreEntities function (the main batch processor):**
```typescript
interface ScoringEvent {
  type: "column_start" | "progress" | "column_complete" | "complete" | "error";
  columnId?: string;
  columnName?: string;
  entityId?: string;
  score?: number | null;
  reasoning?: string;
  response?: string;
  scored?: number;
  total?: number;
}

async function* scoreEntities(
  scanner: IScanner,
  entities: Array<Record<string, unknown>>,
): AsyncGenerator<ScoringEvent>
```

Uses an async generator pattern for cleaner SSE integration. For each AI column in scanner.aiColumns:
1. Yield `{ type: "column_start", columnId, columnName, total: entities.length }`
2. Build the system prompt: `buildScoringSystemPrompt(scanner, column.prompt)`
3. Use p-limit(5) to score entities concurrently
4. For each scored entity, yield `{ type: "progress", columnId, entityId, score, reasoning, response, scored, total }`
5. After all entities for this column, yield `{ type: "column_complete", columnId, scored }`
6. After all columns, yield `{ type: "complete" }`

Error handling per entity: catch errors, yield `{ type: "error", columnId, entityId }`, continue with next entity. Do not let one failure block the batch.

**Install p-limit** if not already: `npm install p-limit` (may already be installed from old plan).
  </action>
  <verify>
Run `npx next build` to verify no TypeScript errors. Check that all functions are exported correctly. Verify p-limit is in package.json.
  </verify>
  <done>
Scoring engine provides buildScoringSystemPrompt, buildEntityUserPrompt, scoreOneEntity, and scoreEntities functions. System prompt exceeds 4096 tokens for Haiku prompt caching. Batch processing uses p-limit(5) concurrency. Async generator yields structured events for SSE streaming. Per-entity error handling ensures resilience. Build passes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Batch scoring SSE API endpoint for scanners</name>
  <files>src/app/api/scanners/[id]/score/route.ts</files>
  <action>
**Create scoring SSE endpoint** at `src/app/api/scanners/[id]/score/route.ts`:

POST handler that streams scoring progress via SSE. Adapts the old `/api/vibe-scanner/score/route.ts` pattern to the new multi-scanner architecture.

**Request flow:**
1. `auth()` for userId, return 401 if not authenticated
2. Extract scanner id from params (await params in Next.js 16)
3. `dbConnect()`
4. Load scanner by id and verify userId matches. Return 404 if not found.
5. Load entities based on scanner type:
   - `rfp`: `Contract.find({}).select('title description buyerName sector valueMin valueMax buyerRegion cpvCodes deadlineDate').lean()`
   - `meetings`: `Signal.find({}).select('organizationName signalType title insight sector sourceDate').lean()`
   - `buyers`: `Buyer.find({}).select('name sector region description contractCount website contacts').lean()`
6. Clear existing scores for this scanner: `Scanner.updateOne({ _id: scanner._id }, { $set: { scores: [] } })`

**SSE stream:**
```typescript
const encoder = new TextEncoder();
const stream = new ReadableStream({
  async start(controller) {
    const allScores: Array<{
      columnId: string;
      entityId: string;
      score: number | null;
      response: string;
      reasoning: string;
      scoredAt: Date;
    }> = [];

    for await (const event of scoreEntities(scanner, entities)) {
      // Stream each event to the client
      controller.enqueue(
        encoder.encode(`data: ${JSON.stringify(event)}\n\n`)
      );

      // Collect scores for persistence
      if (event.type === "progress" && event.entityId) {
        allScores.push({
          columnId: event.columnId!,
          entityId: event.entityId,
          score: event.score ?? null,
          response: event.response || "",
          reasoning: event.reasoning || "",
          scoredAt: new Date(),
        });
      }
    }

    // Persist all scores to MongoDB
    await Scanner.updateOne(
      { _id: scanner._id },
      {
        $set: {
          scores: allScores,
          lastScoredAt: new Date(),
        },
      }
    );

    // Also update the source documents' vibeScore/vibeReasoning for dashboard display
    // Only do this for the first AI column (the primary "Vibe Score" equivalent)
    const firstColumn = scanner.aiColumns[0];
    if (firstColumn) {
      const primaryScores = allScores.filter(s => s.columnId === firstColumn.columnId && s.score != null);
      if (primaryScores.length > 0) {
        const Model = scanner.type === "rfp" ? Contract :
                      scanner.type === "meetings" ? Signal :
                      Buyer;
        const bulkOps = primaryScores.map(s => ({
          updateOne: {
            filter: { _id: s.entityId },
            update: { $set: { vibeScore: s.score, vibeReasoning: s.reasoning } },
          },
        }));
        await Model.bulkWrite(bulkOps);
      }
    }

    controller.close();
  },
});
```

Return `new Response(stream, { headers: { "Content-Type": "text/event-stream", "Cache-Control": "no-cache", "Connection": "keep-alive" } })`.

**Error handling:** Wrap the entire function in try/catch. On catastrophic failure, return JSON error response (not SSE).

**Import models:** Import Contract, Signal, Buyer models and use the correct one based on scanner.type.
  </action>
  <verify>
Run `npx next build` to verify no TypeScript errors. Verify the endpoint file exists at the correct nested route path. Check that SSE response headers are correct.
  </verify>
  <done>
SSE scoring endpoint loads scanner and type-appropriate entities, scores all entities across all AI columns using the scoring engine, streams progress events, persists scores to Scanner document, and updates source documents for dashboard display. Error handling per entity prevents batch failures. Build passes.
  </done>
</task>

</tasks>

<verification>
1. `npx next build` succeeds with no errors
2. Scoring engine exports all functions with correct types
3. SSE endpoint streams events for each column and entity
4. Prompt caching is enabled via cache_control on system message
5. p-limit(5) limits concurrent API calls
6. Scores persist to Scanner.scores in MongoDB
7. Source documents get vibeScore/vibeReasoning from primary AI column
</verification>

<success_criteria>
- Claude Haiku 4.5 scores entities with prompt caching enabled
- SSE streams per-entity progress for each AI column separately
- Concurrent scoring with p-limit(5) stays within rate limits
- Failed individual scores don't block the batch
- Scores persist to both Scanner document and source entity documents
- VIBE-04 (batch scoring), VIBE-10 (progress display) core requirements satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/05-vibe-scanner/05-04-SUMMARY.md`
</output>
