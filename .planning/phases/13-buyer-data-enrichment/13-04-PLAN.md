---
phase: 13-buyer-data-enrichment
plan: 04
type: execute
wave: 4
depends_on: ["13-03"]
files_modified:
  - workers/enrichment/src/stages/04-scrape.ts
  - workers/enrichment/src/stages/05-personnel.ts
  - workers/enrichment/src/db/key-personnel.ts
  - workers/enrichment/src/enrichment-engine.ts
autonomous: true

must_haves:
  truths:
    - "Stage 4 scrapes governance pages for NHS trusts, ICBs, and non-ModernGov councils"
    - "Stage 5 extracts key personnel from governance page text using Claude Haiku"
    - "KeyPersonnel records stored with confidence scores and extraction method"
    - "Non-ModernGov buyers have board documents from HTML scraping"
    - "Claude Haiku extracts procurement-relevant roles (directors, procurement leads, CFOs)"
  artifacts:
    - path: "workers/enrichment/src/stages/04-scrape.ts"
      provides: "HTML scraping for NHS trust/ICB/non-MG council governance pages"
      min_lines: 60
    - path: "workers/enrichment/src/stages/05-personnel.ts"
      provides: "Claude Haiku extraction of key personnel from page text"
      min_lines: 60
    - path: "workers/enrichment/src/db/key-personnel.ts"
      provides: "KeyPersonnel upsert and query operations"
      min_lines: 25
  key_links:
    - from: "workers/enrichment/src/stages/05-personnel.ts"
      to: "@anthropic-ai/sdk"
      via: "Claude Haiku messages.create"
      pattern: "claude-haiku"
    - from: "workers/enrichment/src/stages/04-scrape.ts"
      to: "workers/enrichment/src/db/board-documents.ts"
      via: "upsertBoardDocuments"
      pattern: "upsertBoardDocuments"
    - from: "workers/enrichment/src/stages/05-personnel.ts"
      to: "workers/enrichment/src/db/key-personnel.ts"
      via: "upsertKeyPersonnel"
      pattern: "upsertKeyPersonnel"
---

<objective>
Implement Stage 4 (website scraping for non-ModernGov governance pages -- NHS trusts, ICBs, CMIS councils, custom portals) and Stage 5 (Claude Haiku extraction of key personnel from scraped page text).

Purpose: Stage 3 only covers ModernGov councils (~85-90% of 317 councils). Stage 4 covers the remaining councils plus all NHS trusts (214), ICBs (42), and other Tier 0 orgs that have governance portals but no standard API. Stage 5 then uses AI to extract structured personnel data from these pages.

Output: Website scraping stage, Claude Haiku personnel extraction stage, and KeyPersonnel DB operations.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-buyer-data-enrichment/13-RESEARCH.md
@.planning/phases/13-buyer-data-enrichment/13-03-SUMMARY.md

# Worker patterns
@workers/enrichment/src/enrichment-engine.ts
@workers/enrichment/src/stages/03-moderngov.ts
@workers/enrichment/src/api-clients/rate-limiter.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Stage 4 (website scraping) and KeyPersonnel DB operations</name>
  <files>
workers/enrichment/src/stages/04-scrape.ts
workers/enrichment/src/db/key-personnel.ts
workers/enrichment/src/enrichment-engine.ts
  </files>
  <action>
**workers/enrichment/src/db/key-personnel.ts:**
- upsertKeyPersonnel(db, personnel: Array<Partial<KeyPersonnelDoc>>): Bulk upsert using compound key { buyerId, name }. Use $setOnInsert for creation fields and $set for update fields.
- getKeyPersonnelCount(db, buyerId): Count personnel for a buyer
- getKeyPersonnel(db, buyerId, limit): Fetch key personnel sorted by role priority (chief_executive > director > board_member > procurement_lead > finance_director)

**workers/enrichment/src/stages/04-scrape.ts:**

```typescript
export async function scrapeGovernancePages(
  db: Db, env: Env, job: EnrichmentJobDoc, maxItems: number
): Promise<{ processed: number; errors: number; done: boolean }>
```

Logic:
1. Process buyers in batches of 20 (each buyer requires HTTP calls):
   - Filter: buyers WHERE orgType IS IN ["nhs_trust_acute", "nhs_trust_mental_health", "nhs_trust_community", "nhs_trust_ambulance", "nhs_icb", "fire_rescue", "police_pcc", "combined_authority", "national_park"] OR (orgType starts with "local_council" AND democracyPlatform IN ["CMIS", "Custom", "Jadu", "None"])
   - AND enrichmentSources does NOT contain "scrape"
   - AND (boardPapersUrl is set OR website is set)

2. For each buyer:
   a. Determine scrape target URLs (in priority order):
      - boardPapersUrl if set (direct governance page)
      - democracyPortalUrl if set
      - website + "/about/board" or "/about/governance" (common NHS patterns)
      - website + "/council/committees" (common council fallback)
   b. Fetch each URL using fetchWithDomainDelay (rate limited per domain)
   c. Set 10-second timeout per fetch, skip on timeout/error
   d. Extract meaningful text from HTML response:
      - Strip script/style tags
      - Extract text content from body
      - Truncate to 15,000 chars (enough for Claude but not excessive)
   e. If governance page found and has substantial content (>500 chars):
      - Store as BoardDocument with documentType "board_pack" and extractionStatus "extracted"
      - Store the extracted text in textContent (truncated to 10,000 chars for MongoDB)
   f. Add "scrape" to buyer.enrichmentSources
   g. Update buyer.lastEnrichedAt

3. HTML text extraction helper function:
```typescript
function extractTextFromHtml(html: string): string {
  // Remove script and style blocks
  let text = html.replace(/<script[\s\S]*?<\/script>/gi, "");
  text = text.replace(/<style[\s\S]*?<\/style>/gi, "");
  // Remove HTML tags
  text = text.replace(/<[^>]+>/g, " ");
  // Collapse whitespace
  text = text.replace(/\s+/g, " ").trim();
  return text;
}
```

4. Use per-domain rate limiting (nhs.uk: 2s, gov.uk: 1s, default: 3s)
5. Log failures but continue processing (resilient pipeline)

Wire Stage 4 into enrichment-engine.ts.
  </action>
  <verify>
```bash
cd workers/enrichment && npx tsc --noEmit
```
Verify Stage 4 handles the correct orgTypes:
```bash
grep "nhs_trust" workers/enrichment/src/stages/04-scrape.ts
grep "CMIS\|Custom\|Jadu" workers/enrichment/src/stages/04-scrape.ts
```
  </verify>
  <done>
- Stage 4 scrapes governance pages for all non-ModernGov Tier 0 orgs
- HTML stripped to plain text for downstream Claude Haiku extraction
- BoardDocument records created from scraped governance pages
- KeyPersonnel DB operations support bulk upsert with { buyerId, name } dedup
- Per-domain rate limiting applied across all scraped domains
- Stage 4 wired into enrichment engine
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Stage 5 (Claude Haiku key personnel extraction)</name>
  <files>
workers/enrichment/src/stages/05-personnel.ts
workers/enrichment/src/enrichment-engine.ts
  </files>
  <action>
**workers/enrichment/src/stages/05-personnel.ts:**

```typescript
export async function extractKeyPersonnel(
  db: Db, env: Env, job: EnrichmentJobDoc, maxItems: number
): Promise<{ processed: number; errors: number; done: boolean }>
```

Logic:
1. Initialize Anthropic client: `new Anthropic({ apiKey: env.ANTHROPIC_API_KEY })`
2. Process buyers in batches of 10 (smaller because each buyer requires an AI call):
   - Filter: buyers WHERE enrichmentSources contains "scrape" OR "moderngov", AND enrichmentSources does NOT contain "personnel"
   - For each buyer:
     a. Fetch the buyer's BoardDocument entries where extractionStatus = "extracted" and textContent exists
     b. Combine text content from up to 3 most recent documents (up to 8,000 chars total)
     c. If no text available, skip this buyer (add "personnel" to enrichmentSources to mark as processed)
     d. Call Claude Haiku with extraction prompt:

```typescript
const response = await anthropic.messages.create({
  model: "claude-haiku-4-5-20250401",
  max_tokens: 1024,
  system: `You are extracting key personnel from UK public sector organization governance pages.
Extract ONLY people with procurement-relevant roles: chief executives, directors, board members, procurement leads, finance directors, treasurers, chairs.
Return a JSON array. If no relevant personnel found, return [].`,
  messages: [{
    role: "user",
    content: `Organization: ${buyer.name}
Organization type: ${buyer.orgType}

Governance page content:
${combinedText.slice(0, 8000)}

Extract key personnel as a JSON array with fields:
- name (string, required)
- title (string, job title as written)
- role (one of: chief_executive, director, board_member, procurement_lead, finance_director, cfo, chair, councillor, committee_chair)
- department (string, if mentioned)
- email (string, if found on page)
- confidence (number 0-100, how certain this extraction is correct)`
  }],
});
```

     e. Parse JSON array from response (extract with regex /\[[\s\S]*\]/ then JSON.parse, handle failures)
     f. For each extracted person, create KeyPersonnel document:
        - buyerId, name, title, role, department, email from extraction
        - extractionMethod: "claude_haiku"
        - confidence from extraction
        - sourceUrl: first BoardDocument's sourceUrl
     g. Upsert KeyPersonnel records
     h. Add "personnel" to buyer.enrichmentSources

3. Use p-limit(2) for concurrent Claude Haiku calls (max 2 concurrent per batch to avoid rate limits)
4. Handle Claude API errors gracefully: log and continue, increment errors count
5. Save cursor after each batch

Wire Stage 5 into enrichment-engine.ts.

**Cost awareness:** ~676 Tier 0 orgs at ~$0.01/org = ~$7 per run. Log cost estimate per batch.
  </action>
  <verify>
```bash
cd workers/enrichment && npx tsc --noEmit
```
Verify Claude Haiku integration:
```bash
grep "claude-haiku" workers/enrichment/src/stages/05-personnel.ts
grep "extractionMethod.*claude_haiku" workers/enrichment/src/stages/05-personnel.ts
```
  </verify>
  <done>
- Stage 5 extracts key personnel from governance page text using Claude Haiku
- Extraction prompt targets procurement-relevant roles with structured JSON output
- KeyPersonnel records include name, title, role, confidence, and extractionMethod
- p-limit(2) limits concurrent Claude API calls
- Errors handled gracefully (log, skip buyer, continue)
- Stage 5 wired into enrichment engine
  </done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `cd workers/enrichment && npx tsc --noEmit`
2. Stage 4 targets correct orgTypes (NHS trusts, ICBs, non-MG councils)
3. Stage 5 uses Claude Haiku with proper extraction prompt
4. Both stages registered in enrichment engine
5. HTML extraction strips scripts/styles and collapses whitespace
6. KeyPersonnel upsert deduplicates on { buyerId, name }
7. p-limit(2) limits concurrent AI calls
</verification>

<success_criteria>
- Stage 4 scrapes governance pages for NHS trusts, ICBs, and non-ModernGov councils
- Stage 5 extracts structured key personnel data via Claude Haiku
- KeyPersonnel collection populated with procurement-relevant roles
- Both stages resilient to failures (log and continue)
- Rate limiting applied per-domain for scraping, p-limit for AI calls
</success_criteria>

<output>
After completion, create `.planning/phases/13-buyer-data-enrichment/13-04-SUMMARY.md` documenting:
- Stage 4 scraping targets and HTML extraction approach
- Stage 5 Claude Haiku prompt design and extraction fields
- KeyPersonnel record structure
- Error handling and rate limiting details
- Files created
</output>
